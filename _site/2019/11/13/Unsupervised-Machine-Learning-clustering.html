<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Unsupervised Machine Learning: Clustering</title>
  <meta name="description" content="Identifying the problem:">

  <link rel="stylesheet" href="/Unarine_Tshiwawa_Blog/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/Unarine_Tshiwawa_Blog/2019/11/13/Unsupervised-Machine-Learning-clustering.html">
  <link rel="alternate" type="application/rss+xml" title="Unarine Tshiwawa" href="http://yourdomain.com/Unarine_Tshiwawa_Blog/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/Unarine_Tshiwawa_Blog/">Unarine Tshiwawa</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/Unarine_Tshiwawa_Blog/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Unsupervised Machine Learning: Clustering</h1>
    <p class="post-meta"><time datetime="2019-11-13T00:00:00+02:00" itemprop="datePublished">Nov 13, 2019</time> • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Unarine Tshiwawa</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h3 id="identifying-the-problem">Identifying the problem:</h3>

<p>In this task, I seek to demonstrate how unsupervised learning machine leaning can be used in an unlabelled dataset.</p>

<p><code class="highlighter-rouge">In this work</code></p>

<p>The dataset contained different clusters of stars from some field.  I will use clustering algorithm t attempts to find distinct groups of data without reference to any labels. A powerful clustering method called a Gaussian mixture model (GMM) will be adopted to achieve the goal.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<h1 id="dataset">Dataset</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data.csv'</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s check the shape of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(18061, 160)
</code></pre></div></div>

<p>In this case, we see that we have 18061 samples and each sample in the dataset has 29 features.</p>

<p>Let’s impute missing values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">---------------------------Impute missing values----------------------'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------Impute missing values----------------------



semester                                    0
FieldHalf                                   0
FIELD_1                                     0
Half                                        0
CHIP                                        0
RA_1                                      546
DEC_1                                     546
X_1                                       546
Y_1                                       546
MAG                                       546
MAGe                                      546
Name_1                                      0
Mean_Mag_1                                  0
RMS_1                                       0
Expected_RMS_1                              0
Alarm_2                                     0
Chi2_3                                      0
Jstet_4                                     0
Kurtosis_4                                  0
Lstet_4                                     0
LS_Period_1_5                               0
Log10_LS_Prob_1_5                           0
LS_SNR_1_5                                  0
Killharm_Mean_Mag_6                         0
Killharm_Per1_Fundamental_Sincoeff_6        0
Killharm_Per1_Fundamental_Coscoeff_6        0
Killharm_Per1_Amplitude_6                   0
Period_1_7                                  0
AOV_1_7                                     0
AOV_SNR_1_7                                 0
                                        ...  
rmagAB                                  12476
e_rmag                                  12476
chir                                    12207
warningr                                12021
rmagap                                  12242
rmagapAB                                12242
e_rmagap                                12242
snrr                                    12220
rmaglim                                 12219
PSFFWHMr                                12021
MJDr                                    12021
detIDr                                  12021
cleani                                  12021
imag                                    12707
imagAB                                  12707
e_imag                                  12707
chii                                    12222
warningi                                12021
imagap                                  12252
imagapAB                                12252
e_imagap                                12252
snri                                    12222
imaglim                                 12220
PSFFWHMi                                12021
MJDi                                    12021
detIDi                                  12021
Field_2                                 12021
Ext                                     12021
nbDist                                  12021
Separation                              12021
Length: 160, dtype: int64
</code></pre></div></div>

<p>Here we will remove redundant columns from the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">5</span><span class="p">:</span><span class="mi">34</span><span class="p">]</span>

<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Name_1</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>...</th>
      <th>Killharm_Per1_Fundamental_Sincoeff_6</th>
      <th>Killharm_Per1_Fundamental_Coscoeff_6</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>RA2000</th>
      <th>DEC2000</th>
      <th>lspermin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>100009</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>...</td>
      <td>0.01155</td>
      <td>0.00484</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>17:50:33.0</td>
      <td>-32:30:19.8</td>
      <td>113.177390</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>100218</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>...</td>
      <td>0.00211</td>
      <td>0.00338</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>17:50:16.4</td>
      <td>-32:28:22.3</td>
      <td>92.062210</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>200510</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>...</td>
      <td>-0.26668</td>
      <td>-0.23009</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>17:50:13.3</td>
      <td>-32:17:09.3</td>
      <td>104.545224</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>100445</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>...</td>
      <td>-0.05318</td>
      <td>0.15253</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>17:51:02.1</td>
      <td>-32:29:06.5</td>
      <td>18.891792</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>100546</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>...</td>
      <td>-0.00545</td>
      <td>-0.00253</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>17:51:11.3</td>
      <td>-32:28:20.4</td>
      <td>71.308296</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>200327</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>...</td>
      <td>-0.00799</td>
      <td>-0.02875</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>17:51:04.1</td>
      <td>-32:22:06.4</td>
      <td>97.136510</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>200346</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>...</td>
      <td>-0.01641</td>
      <td>-0.05696</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>17:51:04.4</td>
      <td>-32:22:01.8</td>
      <td>92.754403</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>200687</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>...</td>
      <td>0.04113</td>
      <td>-0.03545</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>17:50:53.0</td>
      <td>-32:20:16.2</td>
      <td>31.794682</td>
    </tr>
    <tr>
      <th>8</th>
      <td>267.782309</td>
      <td>-32.314028</td>
      <td>1216.120</td>
      <td>3348.355</td>
      <td>17.2033</td>
      <td>0.0518</td>
      <td>200950</td>
      <td>21.85581</td>
      <td>0.59356</td>
      <td>0.12884</td>
      <td>...</td>
      <td>-0.57910</td>
      <td>-0.02658</td>
      <td>1.15941</td>
      <td>0.005993</td>
      <td>3.53978</td>
      <td>4.56843</td>
      <td>2.42907</td>
      <td>17:51:07.8</td>
      <td>-32:18:50.5</td>
      <td>110.145859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>267.835602</td>
      <td>-32.319098</td>
      <td>1976.117</td>
      <td>3264.169</td>
      <td>17.4567</td>
      <td>0.0626</td>
      <td>200985</td>
      <td>22.73157</td>
      <td>0.33644</td>
      <td>0.29747</td>
      <td>...</td>
      <td>-0.01420</td>
      <td>-0.29635</td>
      <td>0.59337</td>
      <td>0.006781</td>
      <td>4.54292</td>
      <td>6.00984</td>
      <td>3.99572</td>
      <td>17:51:20.5</td>
      <td>-32:19:08.8</td>
      <td>8.191454</td>
    </tr>
    <tr>
      <th>10</th>
      <td>267.784256</td>
      <td>-32.303904</td>
      <td>1243.385</td>
      <td>3519.108</td>
      <td>16.6806</td>
      <td>0.0357</td>
      <td>200831</td>
      <td>22.14780</td>
      <td>1.11575</td>
      <td>0.40317</td>
      <td>...</td>
      <td>-0.31412</td>
      <td>-0.13794</td>
      <td>0.68615</td>
      <td>0.005668</td>
      <td>9.63083</td>
      <td>5.27875</td>
      <td>10.05811</td>
      <td>17:51:08.2</td>
      <td>-32:18:14.1</td>
      <td>108.213480</td>
    </tr>
    <tr>
      <th>11</th>
      <td>267.850993</td>
      <td>-32.453306</td>
      <td>57.513</td>
      <td>997.950</td>
      <td>12.4843</td>
      <td>0.0038</td>
      <td>101424</td>
      <td>16.03348</td>
      <td>0.00829</td>
      <td>0.00286</td>
      <td>...</td>
      <td>-0.00380</td>
      <td>0.00936</td>
      <td>0.02021</td>
      <td>0.011434</td>
      <td>3.21058</td>
      <td>4.05789</td>
      <td>1.82369</td>
      <td>17:51:24.2</td>
      <td>-32:27:11.9</td>
      <td>99.660254</td>
    </tr>
    <tr>
      <th>12</th>
      <td>267.870488</td>
      <td>-32.362186</td>
      <td>333.339</td>
      <td>2535.851</td>
      <td>15.7359</td>
      <td>0.0212</td>
      <td>200715</td>
      <td>19.72227</td>
      <td>0.02835</td>
      <td>0.02617</td>
      <td>...</td>
      <td>0.02391</td>
      <td>-0.02002</td>
      <td>0.06237</td>
      <td>0.006339</td>
      <td>3.29613</td>
      <td>3.83408</td>
      <td>1.89724</td>
      <td>17:51:28.9</td>
      <td>-32:21:43.9</td>
      <td>12.498826</td>
    </tr>
    <tr>
      <th>13</th>
      <td>267.957741</td>
      <td>-32.282199</td>
      <td>1576.479</td>
      <td>3886.274</td>
      <td>13.7495</td>
      <td>0.0070</td>
      <td>202052</td>
      <td>17.41841</td>
      <td>0.00899</td>
      <td>0.00613</td>
      <td>...</td>
      <td>0.00775</td>
      <td>0.00741</td>
      <td>0.02145</td>
      <td>0.015871</td>
      <td>4.35075</td>
      <td>5.60937</td>
      <td>3.50549</td>
      <td>17:51:49.9</td>
      <td>-32:16:55.9</td>
      <td>97.136510</td>
    </tr>
    <tr>
      <th>14</th>
      <td>267.956182</td>
      <td>-32.282561</td>
      <td>1554.249</td>
      <td>3880.173</td>
      <td>16.3781</td>
      <td>0.0324</td>
      <td>201521</td>
      <td>21.25057</td>
      <td>0.40888</td>
      <td>0.10962</td>
      <td>...</td>
      <td>-0.43060</td>
      <td>0.14495</td>
      <td>0.90868</td>
      <td>0.034771</td>
      <td>6.56148</td>
      <td>7.70614</td>
      <td>6.37014</td>
      <td>17:51:49.5</td>
      <td>-32:16:57.2</td>
      <td>91.380269</td>
    </tr>
  </tbody>
</table>
<p>15 rows × 29 columns</p>
</div>

<p>Let’s now check the shape of the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(18061, 29)
</code></pre></div></div>

<h4 id="removing-all-rows-containing-nans">Removing all rows containing nans:</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RA_1                                    546
DEC_1                                   546
X_1                                     546
Y_1                                     546
MAG                                     546
MAGe                                    546
Name_1                                    0
Mean_Mag_1                                0
RMS_1                                     0
Expected_RMS_1                            0
Alarm_2                                   0
Chi2_3                                    0
Jstet_4                                   0
Kurtosis_4                                0
Lstet_4                                   0
LS_Period_1_5                             0
Log10_LS_Prob_1_5                         0
LS_SNR_1_5                                0
Killharm_Mean_Mag_6                       0
Killharm_Per1_Fundamental_Sincoeff_6      0
Killharm_Per1_Fundamental_Coscoeff_6      0
Killharm_Per1_Amplitude_6                 0
Period_1_7                                0
AOV_1_7                                   0
AOV_SNR_1_7                               0
AOV_NEG_LN_FAP_1_7                        0
RA2000                                  546
DEC2000                                 546
lspermin                                  0
dtype: int64
</code></pre></div></div>

<p>Let’s drop all rows/samples containing nans:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">df</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(17515, 29)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Name_1</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>...</th>
      <th>Killharm_Per1_Fundamental_Sincoeff_6</th>
      <th>Killharm_Per1_Fundamental_Coscoeff_6</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>RA2000</th>
      <th>DEC2000</th>
      <th>lspermin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>100009</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>...</td>
      <td>0.01155</td>
      <td>0.00484</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>17:50:33.0</td>
      <td>-32:30:19.8</td>
      <td>113.177390</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>100218</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>...</td>
      <td>0.00211</td>
      <td>0.00338</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>17:50:16.4</td>
      <td>-32:28:22.3</td>
      <td>92.062210</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>200510</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>...</td>
      <td>-0.26668</td>
      <td>-0.23009</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>17:50:13.3</td>
      <td>-32:17:09.3</td>
      <td>104.545224</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>100445</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>...</td>
      <td>-0.05318</td>
      <td>0.15253</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>17:51:02.1</td>
      <td>-32:29:06.5</td>
      <td>18.891792</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>100546</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>...</td>
      <td>-0.00545</td>
      <td>-0.00253</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>17:51:11.3</td>
      <td>-32:28:20.4</td>
      <td>71.308296</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>200327</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>...</td>
      <td>-0.00799</td>
      <td>-0.02875</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>17:51:04.1</td>
      <td>-32:22:06.4</td>
      <td>97.136510</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>200346</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>...</td>
      <td>-0.01641</td>
      <td>-0.05696</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>17:51:04.4</td>
      <td>-32:22:01.8</td>
      <td>92.754403</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>200687</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>...</td>
      <td>0.04113</td>
      <td>-0.03545</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>17:50:53.0</td>
      <td>-32:20:16.2</td>
      <td>31.794682</td>
    </tr>
    <tr>
      <th>8</th>
      <td>267.782309</td>
      <td>-32.314028</td>
      <td>1216.120</td>
      <td>3348.355</td>
      <td>17.2033</td>
      <td>0.0518</td>
      <td>200950</td>
      <td>21.85581</td>
      <td>0.59356</td>
      <td>0.12884</td>
      <td>...</td>
      <td>-0.57910</td>
      <td>-0.02658</td>
      <td>1.15941</td>
      <td>0.005993</td>
      <td>3.53978</td>
      <td>4.56843</td>
      <td>2.42907</td>
      <td>17:51:07.8</td>
      <td>-32:18:50.5</td>
      <td>110.145859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>267.835602</td>
      <td>-32.319098</td>
      <td>1976.117</td>
      <td>3264.169</td>
      <td>17.4567</td>
      <td>0.0626</td>
      <td>200985</td>
      <td>22.73157</td>
      <td>0.33644</td>
      <td>0.29747</td>
      <td>...</td>
      <td>-0.01420</td>
      <td>-0.29635</td>
      <td>0.59337</td>
      <td>0.006781</td>
      <td>4.54292</td>
      <td>6.00984</td>
      <td>3.99572</td>
      <td>17:51:20.5</td>
      <td>-32:19:08.8</td>
      <td>8.191454</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 29 columns</p>
</div>

<p>We still have three redundant columns, let’s remove them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'RA2000'</span><span class="p">,</span><span class="s">'DEC2000'</span><span class="p">,</span> <span class="s">'Name_1'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------------------Shape of the dataset---------------------------------'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----------------------------------Shape of the dataset---------------------------------



(17515, 26)
</code></pre></div></div>

<p>This dataset is now 21 dimensional: there are 21 features describing each sample.  Here we will use principal component analysis (PCA), which is a fast linear dimensionality reduction technique. It reduces the dimensions of a dataset by projecting the data onto a lower-dimensional subspace.</p>

<p>For 2-dimensional subspace: The first principal component (PCA1) covers the maximum variance in the data and the second
principal component (PCA2) is orthogonal to the first principal component–all principal components are orthogonal to each other.</p>

<p>A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.  The cumulative explained variance technique, which measures how well PCA preserves the content of the data will be employed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">data_rescaled</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Variance (</span><span class="si">%</span><span class="s">)'</span><span class="p">)</span> <span class="c">#for each component</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Cumulative explained variance'</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Cumulative explained variance')
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_21_1.png?token=AE5UC7YSVCYYLMNEKNCSYOS54F3YE" alt="" /></p>

<p>Here we see that our two-dimensional projection will lose a lot of information (as measured by the explained variance) and that we’d need about 6 components to retain about 90$\%$ of the variance.</p>

<p>The curve above quantifies how much of the total, 21-dimensional variance is contained within the first N components.</p>

<p><code class="highlighter-rouge">We will convert our dataset to two dimension:</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c">#project from 29 to 2 dimensions</span>

<span class="n">x_2D</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c">#Transform the data to two dimensions</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_2D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_2D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>Alarm_2</th>
      <th>...</th>
      <th>Killharm_Per1_Fundamental_Sincoeff_6</th>
      <th>Killharm_Per1_Fundamental_Coscoeff_6</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>lspermin</th>
      <th>PCA1</th>
      <th>PCA2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>6.39196</td>
      <td>...</td>
      <td>0.01155</td>
      <td>0.00484</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>113.177390</td>
      <td>1948.591969</td>
      <td>337.968307</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>5.10836</td>
      <td>...</td>
      <td>0.00211</td>
      <td>0.00338</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>92.062210</td>
      <td>1419.477850</td>
      <td>-657.317068</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>7.80936</td>
      <td>...</td>
      <td>-0.26668</td>
      <td>-0.23009</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>104.545224</td>
      <td>-1729.683459</td>
      <td>-913.293074</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>0.66387</td>
      <td>...</td>
      <td>-0.05318</td>
      <td>0.15253</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>18.891792</td>
      <td>1611.715190</td>
      <td>-90.758327</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>3.85503</td>
      <td>...</td>
      <td>-0.00545</td>
      <td>-0.00253</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>71.308296</td>
      <td>1384.455794</td>
      <td>454.317422</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>5.04427</td>
      <td>...</td>
      <td>-0.00799</td>
      <td>-0.02875</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>97.136510</td>
      <td>-357.867313</td>
      <td>-10.641048</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>2.51186</td>
      <td>...</td>
      <td>-0.01641</td>
      <td>-0.05696</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>92.754403</td>
      <td>-380.040359</td>
      <td>4.908457</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>1.10537</td>
      <td>...</td>
      <td>0.04113</td>
      <td>-0.03545</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>31.794682</td>
      <td>-860.374826</td>
      <td>-681.993746</td>
    </tr>
    <tr>
      <th>8</th>
      <td>267.782309</td>
      <td>-32.314028</td>
      <td>1216.120</td>
      <td>3348.355</td>
      <td>17.2033</td>
      <td>0.0518</td>
      <td>21.85581</td>
      <td>0.59356</td>
      <td>0.12884</td>
      <td>10.07342</td>
      <td>...</td>
      <td>-0.57910</td>
      <td>-0.02658</td>
      <td>1.15941</td>
      <td>0.005993</td>
      <td>3.53978</td>
      <td>4.56843</td>
      <td>2.42907</td>
      <td>110.145859</td>
      <td>-1279.778915</td>
      <td>184.983958</td>
    </tr>
    <tr>
      <th>9</th>
      <td>267.835602</td>
      <td>-32.319098</td>
      <td>1976.117</td>
      <td>3264.169</td>
      <td>17.4567</td>
      <td>0.0626</td>
      <td>22.73157</td>
      <td>0.33644</td>
      <td>0.29747</td>
      <td>-0.58983</td>
      <td>...</td>
      <td>-0.01420</td>
      <td>-0.29635</td>
      <td>0.59337</td>
      <td>0.006781</td>
      <td>4.54292</td>
      <td>6.00984</td>
      <td>3.99572</td>
      <td>8.191454</td>
      <td>-1209.589339</td>
      <td>946.396651</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 28 columns</p>
</div>

<h1 id="visualise-data-in-2d">Visualise data in 2D</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_28_1.png?token=AE5UC76V6UGBCZZGYLGN2NC54F4F2" alt="" /></p>

<p>Clearly, we see that 2D doesn’t give  much information.</p>

<h3 id="lets-consider-two-clusterring-algorithms-kmeans--gaussian-mixture">Let’s consider two clusterring algorithms (KMeans &amp; Gaussian Mixture):</h3>

<p>We will instantiates different algorithms</p>

<h1 id="1-k-means">1. K-Means</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>


</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="optimal-number-of-clusters-and-cluster-evaluation">Optimal number of clusters and cluster evaluation:</h2>

<p>We shall use <code class="highlighter-rouge">the elbow method</code> to determine the optimal number of clusters in k-means clustering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span><span class="p">,</span> <span class="n">pdist</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c"># Avg. within-cluster sum of squares</span>

<span class="c">#</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">model1</span> <span class="o">=</span> <span class="p">[</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">]</span>

<span class="c">#centroids</span>
<span class="n">cent_</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">cluster_centers_</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">model1</span><span class="p">]</span>

<span class="c">#Compute distance between each pair of the two collections of inputs</span>
<span class="n">D_k</span> <span class="o">=</span> <span class="p">[</span><span class="n">cdist</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">,</span> <span class="n">centrds</span><span class="p">,</span> <span class="s">'euclidean'</span><span class="p">)</span> <span class="k">for</span> <span class="n">centrds</span> <span class="ow">in</span> <span class="n">cent_</span><span class="p">]</span>

<span class="c">#compute minimum distance to each centroid</span>
<span class="n">dist</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">D</span> <span class="ow">in</span> <span class="n">D_k</span><span class="p">]</span>

<span class="c">#average distance within cluster-sum</span>
<span class="n">avgWithinSS</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="n">data_rescaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dist</span><span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p><code class="highlighter-rouge">Let's view elbow curve</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># elbow curve - Avg. within-cluster sum of squares</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">avgWithinSS</span><span class="p">,</span> <span class="s">'b*-'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of clusters'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Average within-cluster sum of squares'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'Average within-cluster sum of squares')
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_39_1.png?token=AE5UC72QEUHNQBRMG2L3HSK54F4JS" alt="" /></p>

<p>The elbow seemed to be located at k = 3.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s">'preprocessor'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">'model'</span><span class="p">,</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">10</span><span class="p">))])</span>

<span class="n">model1</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_kmeans</span>

<span class="n">x</span><span class="o">.</span><span class="n">clusterKM</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0, 2, 1])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_43_1.png?token=AE5UC7Z23MNIOAHEACL7RFS54F4MI" alt="" /></p>

<p>The figure above shows that, the dataset contain 3 groups of stars.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c">#x.clusterGMM.value_counts().plot('bar')</span>


<span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span>
                                 <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                 <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    9622
1    5550
2    2343
Name: clusterKM, dtype: int64
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_45_1.png?token=AE5UC7ZCO3O2NYG2PV26CL254F4QI" alt="" /></p>

<h2 id="2-gaussian-mixture-model">2 Gaussian Mixture Model:</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
</code></pre></div></div>

<h3 id="selecting-the-number-of-components-in-a-classical-gaussian-mixture-mode">Selecting the number of components in a classical Gaussian Mixture Mode.</h3>

<p>We shall use the Bayesian information criterion (BIC) to select the number of components in a Gaussian Mixture in an efficient way.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">gm_bic</span><span class="o">=</span> <span class="p">[]</span>
<span class="n">gm_score</span><span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>

    <span class="n">gm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"BIC for number of cluster(s) {}: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">gm</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Log-likelihood score for number of cluster(s) {}: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">gm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">gm_bic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">gm</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">))</span>
    <span class="n">gm_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">))</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"The Gaussian Mixture model BIC </span><span class="se">\n</span><span class="s">for determining number of clusters</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)],</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gm_bic</span><span class="p">),</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of clusters"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Log of Gaussian mixture BIC score"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BIC for number of cluster(s) 1: -1676190.6216027052
Log-likelihood score for number of cluster(s) 1: 47.95530168653835
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 2: -2171325.82039032
Log-likelihood score for number of cluster(s) 2: 62.195340683891565
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 3: -2300437.126170328
Log-likelihood score for number of cluster(s) 3: 65.98651034046242
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 4: -2328548.6398803927
Log-likelihood score for number of cluster(s) 4: 66.89444299817677
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 5: -2380433.2167969383
Log-likelihood score for number of cluster(s) 5: 68.48102433435189
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 6: -2397149.4573575854
Log-likelihood score for number of cluster(s) 6: 69.06365658814336
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 7: -2417503.796183502
Log-likelihood score for number of cluster(s) 7: 69.75014548667563
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 8: -2445254.568869919
Log-likelihood score for number of cluster(s) 8: 70.64778008490818
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 9: -2437824.68070565
Log-likelihood score for number of cluster(s) 9: 70.54111377390045
----------------------------------------------------------------------------------------------------
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_49_1.png?token=AE5UC73HJSUS2W3LQBZ7PTC54F4UQ" alt="" /></p>

<p>The BIC is clearly maximized at a value of 3.</p>

<p><code class="highlighter-rouge">We shall take 3, as the number of components in the mixture: n_components = 3</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>



<span class="n">GMM</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s">'preprocessor'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">'model'</span><span class="p">,</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">covariance_type</span> <span class="o">=</span> <span class="s">'full'</span><span class="p">))])</span>

<span class="c">#assigning feature matrix to a new variable</span>
<span class="n">xx</span> <span class="o">=</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GMM</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

<span class="c">#print(model.score(x))</span>

<span class="n">y_gmm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xx</span><span class="p">)</span>

<span class="n">xx</span><span class="p">[</span><span class="s">'clusterGMM'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_gmm</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">Let's view our 2D figure:</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xx</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">xx</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_gmm</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_53_1.png?token=AE5UC77JBDHM5NZFLIANMPS54F4W2" alt="" /></p>

<p>The figure above shows that, the dataset contain 3 groups of stars.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">clusterGMM</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c">#x.clusterGMM.value_counts().plot('bar')</span>


<span class="n">xx</span><span class="p">[</span><span class="s">'clusterGMM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span>
                                 <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                 <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1    9622
0    5550
2    2343
Name: clusterGMM, dtype: int64
</code></pre></div></div>

<p><img src="https://raw.githubusercontent.com/tshuna001/images/master/the_stars2_55_1.png?token=AE5UC742L7YK3WVW5L2HHQK54F4ZC" alt="" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>Alarm_2</th>
      <th>...</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>lspermin</th>
      <th>PCA1</th>
      <th>PCA2</th>
      <th>clusterKM</th>
      <th>clusterGMM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>6.39196</td>
      <td>...</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>113.177390</td>
      <td>1948.591969</td>
      <td>337.968307</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>5.10836</td>
      <td>...</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>92.062210</td>
      <td>1419.477850</td>
      <td>-657.317068</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>7.80936</td>
      <td>...</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>104.545224</td>
      <td>-1729.683459</td>
      <td>-913.293074</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>0.66387</td>
      <td>...</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>18.891792</td>
      <td>1611.715190</td>
      <td>-90.758327</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>3.85503</td>
      <td>...</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>71.308296</td>
      <td>1384.455794</td>
      <td>454.317422</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>5.04427</td>
      <td>...</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>97.136510</td>
      <td>-357.867313</td>
      <td>-10.641048</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>2.51186</td>
      <td>...</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>92.754403</td>
      <td>-380.040359</td>
      <td>4.908457</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>1.10537</td>
      <td>...</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>31.794682</td>
      <td>-860.374826</td>
      <td>-681.993746</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 30 columns</p>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Unarine Tshiwawa</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Unarine Tshiwawa</li>
          <li><a href="mailto:unarine.tshiwawa@gmail.com">unarine.tshiwawa@gmail.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/tshuna001"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">tshuna001</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>
"To me success means effectiveness in the world, that I am able to carry my ideas and values into the world—that I am able to change it in positive ways."
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
