I"Dî<script type="math/tex; mode=display">\mathrm{\textbf{Credit Card Fraud Detection}}</script>

<p>Organisation that pride themselves to protect customers or businesses against fraudulent activities have to take the necessary measures to reduce monetary loss, keep customer and brand reputation high while keeping organizational efficiencies on track. Hence, staying ahead of fraudsters and bots impersonating humans.  Organizations from hospitals to banks to government agencies have to manage all of this while meeting strict compliance guidelines, managing data and IT cybersecurity and more. How can this be?  In this work, I will use machine learning method for cradit card fraud detection</p>

<p>I shall layout, as a data scientist, an approach that could be based on two main risk drivers of credit card transaction default prediction:. 1) Non-fraudulent transaction and 2) Fraudulent transaction. I shall demonstrate how to build robust models to effectively predict the odds of transactions.  The system is shown mostly normal instances during training, so it learns to recognize them and when it sees a new instance it can tell whether it looks like a normal one or whether it is likely an anomaly</p>

<p>Clearly, we see that this is a machine leaning - binary classification task - type of a problem where the machine learning algorithm learns a set of rules in order to distinguish between two possible classes: non-fraudulent and fraudulent transactions in credit cards.  The main goal in supervised learning is to learn a model (based on past observations) from labeled training data that allows us to make predictions about unseen or future data of new instances.</p>

<p><strong>Letâ€™s get started:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#get libraries
</span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_columns'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_rows'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="data">Data</h1>

<p>The dataset in this work comes from <a href="https://www.kaggle.com/">kaggle</a>.  The data sets contains transactions made by credit cards by cardholders.  For this credit card fraud dataset, we do not have the original features.  It contains only numerical input variables which are the result of a principal component analysis (PCA) transformation. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, features which have not been transformed with PCA is <code class="highlighter-rouge">Time</code> and <code class="highlighter-rouge">Amount</code>.  For more infomation about the data, the reader is referred to <a href="https://www.kaggle.com/shayannaveed/credit-card-fraud-detection">details</a>.</p>

<p>Read dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">name</span> <span class="o">=</span> <span class="s">'credit-card-fraud-detection.zip'</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">compression</span> <span class="o">=</span> <span class="s">'zip'</span><span class="p">)</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="features-of-each-instance-in-the-dataset">Features of each instance in the dataset:</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',
       'Class'],
      dtype='object')
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(284807, 31)
</code></pre></div></div>

<p>The dataset contains 284807 instances, each contains 31 features.</p>

<h3 id="take-a-quick-look-at-the-data-structure">Take a Quick Look at the Data Structure</h3>

<p>Letâ€™s take a look at the first few rows using the DataFrameâ€™s head() method</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>V11</th>
      <th>V12</th>
      <th>V13</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>0.090794</td>
      <td>-0.551600</td>
      <td>-0.617801</td>
      <td>-0.991390</td>
      <td>-0.311169</td>
      <td>1.468177</td>
      <td>-0.470401</td>
      <td>0.207971</td>
      <td>0.025791</td>
      <td>0.403993</td>
      <td>0.251412</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>-0.166974</td>
      <td>1.612727</td>
      <td>1.065235</td>
      <td>0.489095</td>
      <td>-0.143772</td>
      <td>0.635558</td>
      <td>0.463917</td>
      <td>-0.114805</td>
      <td>-0.183361</td>
      <td>-0.145783</td>
      <td>-0.069083</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>0.207643</td>
      <td>0.624501</td>
      <td>0.066084</td>
      <td>0.717293</td>
      <td>-0.165946</td>
      <td>2.345865</td>
      <td>-2.890083</td>
      <td>1.109969</td>
      <td>-0.121359</td>
      <td>-2.261857</td>
      <td>0.524980</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>-0.054952</td>
      <td>-0.226487</td>
      <td>0.178228</td>
      <td>0.507757</td>
      <td>-0.287924</td>
      <td>-0.631418</td>
      <td>-1.059647</td>
      <td>-0.684093</td>
      <td>1.965775</td>
      <td>-1.232622</td>
      <td>-0.208038</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.0</td>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>0.753074</td>
      <td>-0.822843</td>
      <td>0.538196</td>
      <td>1.345852</td>
      <td>-1.119670</td>
      <td>0.175121</td>
      <td>-0.451449</td>
      <td>-0.237033</td>
      <td>-0.038195</td>
      <td>0.803487</td>
      <td>0.408542</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2.0</td>
      <td>-0.425966</td>
      <td>0.960523</td>
      <td>1.141109</td>
      <td>-0.168252</td>
      <td>0.420987</td>
      <td>-0.029728</td>
      <td>0.476201</td>
      <td>0.260314</td>
      <td>-0.568671</td>
      <td>-0.371407</td>
      <td>1.341262</td>
      <td>0.359894</td>
      <td>-0.358091</td>
      <td>-0.137134</td>
      <td>0.517617</td>
      <td>0.401726</td>
      <td>-0.058133</td>
      <td>0.068653</td>
      <td>-0.033194</td>
      <td>0.084968</td>
      <td>-0.208254</td>
      <td>-0.559825</td>
      <td>-0.026398</td>
      <td>-0.371427</td>
      <td>-0.232794</td>
      <td>0.105915</td>
      <td>0.253844</td>
      <td>0.081080</td>
      <td>3.67</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4.0</td>
      <td>1.229658</td>
      <td>0.141004</td>
      <td>0.045371</td>
      <td>1.202613</td>
      <td>0.191881</td>
      <td>0.272708</td>
      <td>-0.005159</td>
      <td>0.081213</td>
      <td>0.464960</td>
      <td>-0.099254</td>
      <td>-1.416907</td>
      <td>-0.153826</td>
      <td>-0.751063</td>
      <td>0.167372</td>
      <td>0.050144</td>
      <td>-0.443587</td>
      <td>0.002821</td>
      <td>-0.611987</td>
      <td>-0.045575</td>
      <td>-0.219633</td>
      <td>-0.167716</td>
      <td>-0.270710</td>
      <td>-0.154104</td>
      <td>-0.780055</td>
      <td>0.750137</td>
      <td>-0.257237</td>
      <td>0.034507</td>
      <td>0.005168</td>
      <td>4.99</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>7.0</td>
      <td>-0.644269</td>
      <td>1.417964</td>
      <td>1.074380</td>
      <td>-0.492199</td>
      <td>0.948934</td>
      <td>0.428118</td>
      <td>1.120631</td>
      <td>-3.807864</td>
      <td>0.615375</td>
      <td>1.249376</td>
      <td>-0.619468</td>
      <td>0.291474</td>
      <td>1.757964</td>
      <td>-1.323865</td>
      <td>0.686133</td>
      <td>-0.076127</td>
      <td>-1.222127</td>
      <td>-0.358222</td>
      <td>0.324505</td>
      <td>-0.156742</td>
      <td>1.943465</td>
      <td>-1.015455</td>
      <td>0.057504</td>
      <td>-0.649709</td>
      <td>-0.415267</td>
      <td>-0.051634</td>
      <td>-1.206921</td>
      <td>-1.085339</td>
      <td>40.80</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="dataset-information">Dataset information:</h3>

<p>The info() method is useful to get a quick description of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
Time      284807 non-null float64
V1        284807 non-null float64
V2        284807 non-null float64
V3        284807 non-null float64
V4        284807 non-null float64
V5        284807 non-null float64
V6        284807 non-null float64
V7        284807 non-null float64
V8        284807 non-null float64
V9        284807 non-null float64
V10       284807 non-null float64
V11       284807 non-null float64
V12       284807 non-null float64
V13       284807 non-null float64
V14       284807 non-null float64
V15       284807 non-null float64
V16       284807 non-null float64
V17       284807 non-null float64
V18       284807 non-null float64
V19       284807 non-null float64
V20       284807 non-null float64
V21       284807 non-null float64
V22       284807 non-null float64
V23       284807 non-null float64
V24       284807 non-null float64
V25       284807 non-null float64
V26       284807 non-null float64
V27       284807 non-null float64
V28       284807 non-null float64
Amount    284807 non-null float64
Class     284807 non-null int64
dtypes: float64(30), int64(1)
memory usage: 67.4 MB



None
</code></pre></div></div>

<p>There are 284807 instances in the dataset, which means that it is sufficient by Machine Learning standards, and itâ€™s perfect to get started with.  There are no null values in the dataset - thatâ€™s beautiful :)</p>

<p>The feature - <code class="highlighter-rouge">Amount</code> - is the transaction amount, this feature can be used for example-dependant cost-senstive learning. Feature - <code class="highlighter-rouge">Class</code> - is the response variable and <strong>it takes value 1 in case of fraud and 0 otherwise</strong>. <code class="highlighter-rouge">Time</code> is the number of seconds elapsed between this transaction and the first transaction in the dataset.</p>

<p>PCA dimensionality reduction may be as a result to protect user identities and sensitive features(v1-v28)</p>

<h3 id="basic-statistics-summary-of-each-numerical-attribute">Basic statistics: Summary of each numerical attribute.</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>V11</th>
      <th>V12</th>
      <th>V13</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>284807.000000</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>284807.000000</td>
      <td>284807.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>94813.859575</td>
      <td>3.919560e-15</td>
      <td>5.688174e-16</td>
      <td>-8.769071e-15</td>
      <td>2.782312e-15</td>
      <td>-1.552563e-15</td>
      <td>2.010663e-15</td>
      <td>-1.694249e-15</td>
      <td>-1.927028e-16</td>
      <td>-3.137024e-15</td>
      <td>1.768627e-15</td>
      <td>9.170318e-16</td>
      <td>-1.810658e-15</td>
      <td>1.693438e-15</td>
      <td>1.479045e-15</td>
      <td>3.482336e-15</td>
      <td>1.392007e-15</td>
      <td>-7.528491e-16</td>
      <td>4.328772e-16</td>
      <td>9.049732e-16</td>
      <td>5.085503e-16</td>
      <td>1.537294e-16</td>
      <td>7.959909e-16</td>
      <td>5.367590e-16</td>
      <td>4.458112e-15</td>
      <td>1.453003e-15</td>
      <td>1.699104e-15</td>
      <td>-3.660161e-16</td>
      <td>-1.206049e-16</td>
      <td>88.349619</td>
      <td>0.001727</td>
    </tr>
    <tr>
      <th>std</th>
      <td>47488.145955</td>
      <td>1.958696e+00</td>
      <td>1.651309e+00</td>
      <td>1.516255e+00</td>
      <td>1.415869e+00</td>
      <td>1.380247e+00</td>
      <td>1.332271e+00</td>
      <td>1.237094e+00</td>
      <td>1.194353e+00</td>
      <td>1.098632e+00</td>
      <td>1.088850e+00</td>
      <td>1.020713e+00</td>
      <td>9.992014e-01</td>
      <td>9.952742e-01</td>
      <td>9.585956e-01</td>
      <td>9.153160e-01</td>
      <td>8.762529e-01</td>
      <td>8.493371e-01</td>
      <td>8.381762e-01</td>
      <td>8.140405e-01</td>
      <td>7.709250e-01</td>
      <td>7.345240e-01</td>
      <td>7.257016e-01</td>
      <td>6.244603e-01</td>
      <td>6.056471e-01</td>
      <td>5.212781e-01</td>
      <td>4.822270e-01</td>
      <td>4.036325e-01</td>
      <td>3.300833e-01</td>
      <td>250.120109</td>
      <td>0.041527</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>-5.640751e+01</td>
      <td>-7.271573e+01</td>
      <td>-4.832559e+01</td>
      <td>-5.683171e+00</td>
      <td>-1.137433e+02</td>
      <td>-2.616051e+01</td>
      <td>-4.355724e+01</td>
      <td>-7.321672e+01</td>
      <td>-1.343407e+01</td>
      <td>-2.458826e+01</td>
      <td>-4.797473e+00</td>
      <td>-1.868371e+01</td>
      <td>-5.791881e+00</td>
      <td>-1.921433e+01</td>
      <td>-4.498945e+00</td>
      <td>-1.412985e+01</td>
      <td>-2.516280e+01</td>
      <td>-9.498746e+00</td>
      <td>-7.213527e+00</td>
      <td>-5.449772e+01</td>
      <td>-3.483038e+01</td>
      <td>-1.093314e+01</td>
      <td>-4.480774e+01</td>
      <td>-2.836627e+00</td>
      <td>-1.029540e+01</td>
      <td>-2.604551e+00</td>
      <td>-2.256568e+01</td>
      <td>-1.543008e+01</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>54201.500000</td>
      <td>-9.203734e-01</td>
      <td>-5.985499e-01</td>
      <td>-8.903648e-01</td>
      <td>-8.486401e-01</td>
      <td>-6.915971e-01</td>
      <td>-7.682956e-01</td>
      <td>-5.540759e-01</td>
      <td>-2.086297e-01</td>
      <td>-6.430976e-01</td>
      <td>-5.354257e-01</td>
      <td>-7.624942e-01</td>
      <td>-4.055715e-01</td>
      <td>-6.485393e-01</td>
      <td>-4.255740e-01</td>
      <td>-5.828843e-01</td>
      <td>-4.680368e-01</td>
      <td>-4.837483e-01</td>
      <td>-4.988498e-01</td>
      <td>-4.562989e-01</td>
      <td>-2.117214e-01</td>
      <td>-2.283949e-01</td>
      <td>-5.423504e-01</td>
      <td>-1.618463e-01</td>
      <td>-3.545861e-01</td>
      <td>-3.171451e-01</td>
      <td>-3.269839e-01</td>
      <td>-7.083953e-02</td>
      <td>-5.295979e-02</td>
      <td>5.600000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>84692.000000</td>
      <td>1.810880e-02</td>
      <td>6.548556e-02</td>
      <td>1.798463e-01</td>
      <td>-1.984653e-02</td>
      <td>-5.433583e-02</td>
      <td>-2.741871e-01</td>
      <td>4.010308e-02</td>
      <td>2.235804e-02</td>
      <td>-5.142873e-02</td>
      <td>-9.291738e-02</td>
      <td>-3.275735e-02</td>
      <td>1.400326e-01</td>
      <td>-1.356806e-02</td>
      <td>5.060132e-02</td>
      <td>4.807155e-02</td>
      <td>6.641332e-02</td>
      <td>-6.567575e-02</td>
      <td>-3.636312e-03</td>
      <td>3.734823e-03</td>
      <td>-6.248109e-02</td>
      <td>-2.945017e-02</td>
      <td>6.781943e-03</td>
      <td>-1.119293e-02</td>
      <td>4.097606e-02</td>
      <td>1.659350e-02</td>
      <td>-5.213911e-02</td>
      <td>1.342146e-03</td>
      <td>1.124383e-02</td>
      <td>22.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>139320.500000</td>
      <td>1.315642e+00</td>
      <td>8.037239e-01</td>
      <td>1.027196e+00</td>
      <td>7.433413e-01</td>
      <td>6.119264e-01</td>
      <td>3.985649e-01</td>
      <td>5.704361e-01</td>
      <td>3.273459e-01</td>
      <td>5.971390e-01</td>
      <td>4.539234e-01</td>
      <td>7.395934e-01</td>
      <td>6.182380e-01</td>
      <td>6.625050e-01</td>
      <td>4.931498e-01</td>
      <td>6.488208e-01</td>
      <td>5.232963e-01</td>
      <td>3.996750e-01</td>
      <td>5.008067e-01</td>
      <td>4.589494e-01</td>
      <td>1.330408e-01</td>
      <td>1.863772e-01</td>
      <td>5.285536e-01</td>
      <td>1.476421e-01</td>
      <td>4.395266e-01</td>
      <td>3.507156e-01</td>
      <td>2.409522e-01</td>
      <td>9.104512e-02</td>
      <td>7.827995e-02</td>
      <td>77.165000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>172792.000000</td>
      <td>2.454930e+00</td>
      <td>2.205773e+01</td>
      <td>9.382558e+00</td>
      <td>1.687534e+01</td>
      <td>3.480167e+01</td>
      <td>7.330163e+01</td>
      <td>1.205895e+02</td>
      <td>2.000721e+01</td>
      <td>1.559499e+01</td>
      <td>2.374514e+01</td>
      <td>1.201891e+01</td>
      <td>7.848392e+00</td>
      <td>7.126883e+00</td>
      <td>1.052677e+01</td>
      <td>8.877742e+00</td>
      <td>1.731511e+01</td>
      <td>9.253526e+00</td>
      <td>5.041069e+00</td>
      <td>5.591971e+00</td>
      <td>3.942090e+01</td>
      <td>2.720284e+01</td>
      <td>1.050309e+01</td>
      <td>2.252841e+01</td>
      <td>4.584549e+00</td>
      <td>7.519589e+00</td>
      <td>3.517346e+00</td>
      <td>3.161220e+01</td>
      <td>3.384781e+01</td>
      <td>25691.160000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>First thing to notice, the dataset has no null values.  The average amount used is 88.34$\pm$250.12</p>

<h1 id="1-exploratory-data-analasis">1. Exploratory Data Analasis</h1>

<p>Histogram all numerical features in the dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df2</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df2</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1PaQxWI4c1G73L4Ix0KRZhrFGHTvBQhvB" alt="png" /></p>

<p>Mapping numerical feature (<code class="highlighter-rouge">Class</code>) to categorical fearure.  O and 1 indicates all transactions that are non-fradulent and fradulent, respectively.  We are doing this bacause we want to visualise numerical features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df2</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df2</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="nb">map</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span><span class="s">'Non-fraudulent'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="s">'Fraudulent'</span><span class="p">})</span>
<span class="n">df2</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array(['Non-fraudulent', 'Fraudulent'], dtype=object)
</code></pre></div></div>

<p>data viz: pie plot</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df2</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span> <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                         <span class="n">shadow</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">df2</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Non-fraudulent    284315
Fraudulent           492
Name: Class, dtype: int64
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1TQbEyHrxRNggh0J5CvWfBp2WjbCFw398" alt="png" /></p>

<p>Note: <code class="highlighter-rouge">We see that we have huge disparity between Fraudelent and non-fraudulent cases.</code></p>

<p>PCA was already performed on this credit card dataset, to removing the redundancy for us.  Feature selection is not necessary either since the number of observations (284,807) vastly outnumbers the number of features (30), which dramatically reduces the chances of overfitting.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="check-correlation-of-features">Check correlation of features</h2>

<p>The correlation coefficient only measures linear correlations.  Letâ€™s check for linearity amoung features.  So we can easily compute the standard correlation coefficient (also called Pearsonâ€™s r) between every pair of attributes using the corr() method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="n">corr_matrix</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">corr_mat</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1eHocF_Nl37tuWgLk7QAZC3IIWW84Od8P" alt="png" /></p>

<p>The correlation coefficient ranges from â€“1 to 1. When it is close to 1, it means that there is a strong positive correlation.  When the coefficient is close to â€“1, it means that there is a strong negative correlation.  Finally, coefficients close to zero mean that there is no linear correlation.</p>

<p>Info: We do not see any strong correlation among features, so there is no strong correlation among features.  So linear model will not be a good fit here!</p>

<h1 id="2-preprocessing---getting-data-into-shape">2. Preprocessing - getting data into shape</h1>

<p>To determine whether our machine learning algorithm not only performs well on the training set but also generalizes well to new data, we also want to randomly divide the dataset into a separate <code class="highlighter-rouge">training</code> and <code class="highlighter-rouge">test set</code>. <code class="highlighter-rouge">We use the training set to train and optimize our machine learning model</code>, while we keep the <code class="highlighter-rouge">test set until the very end to evaluate the final model</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#feature matrix
</span><span class="n">x</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Class'</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1">#Target vector/labels
</span><span class="n">y</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(284807, 30)
(284807,)
</code></pre></div></div>

<h3 id="data-splicing">Data Splicing</h3>

<p>We are going to split the data into three sets -  80%, 20% for training, validation, respectively.  This means, 80% of the data will be set to train and optimize our machine learning model and 20% will be used to validate the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Letâ€™s verify data partitioning:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">]:</span>

    <span class="k">print</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Done!!&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>80.0 %
20.0 %
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Done!!&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
</code></pre></div></div>

<h3 id="feature-scaling">Feature scaling</h3>

<p><code class="highlighter-rouge">Support vector machines</code> is sensitive to the feature scales.  Data are not usually presented to the machine learning algorithm in exactly the same raw form as it is found. Usually data are scaled to a specific range in a process called normalization for optimal performance.</p>

<p>We are going to make selected features to the same scale for optimal performance, which is often achieved by transforming the features in the range [0, 1]: standardize features by removing the mean and scaling to unit variance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_val_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

</code></pre></div></div>

<h1 id="3-machine-learning-model">3. Machine Learning model</h1>

<p>Now, the feature matrix and labels are ready for training. This is the part where different classifier will be instantiated (for training) and a model which performs better in both training and test set will be used for further predictions.</p>

<p>It is important to note that the parameters for the previously mentioned procedures, such as feature scaling and dimensionality reduction, are solely obtained from the training dataset, and the same parameters are later reapplied to transform the test dataset, as well as any new data samplesâ€”the performance measured on the test data may be overly optimistic otherwise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">LocalOutlierFactor</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>
</code></pre></div></div>

<h3 id="lets-discuss-different-class-of-classifiers-that-will-be-used-to-train-the-model">Letâ€™s discuss different class of classifiers that will be used to train the model:</h3>

<ol>
  <li>
    <p><code class="highlighter-rouge">LocalOutlierFactor</code>: Is an unsupervised outlier detection method.  It calculates an anomaly score of each sample - thatâ€™s called the local outlier factor.  It measure the local deviation of density for a given sample relative to its neighbors.  In other word, this model pin-point how isolated an object is with respect to its surrounding. This method is similar to k-NN, however, we are calcu;ating an anomaly scores based on those neighbors <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html">LocalOutlierFactor</a></p>
  </li>
  <li>
    <p><code class="highlighter-rouge">IsolationForest</code>: Is also an unsupervised detection method.  This is a bit different from LocalOutlierFactor, it isolate the observation by randomly selecting a feature and a split value between maximum and minimum values of the selected feature <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html">IsolationForest</a>.</p>
  </li>
  <li>
    <p><code class="highlighter-rouge">C-Support Vector Classification</code> is a supervised machine learning algorithm capable of performing classification, regression and even outlier detection <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">C-Support Vector Classification</a>.</p>
  </li>
</ol>

<p>### The proportion of outliers in the dataset:</p>

<p>This define the threshold on the scores of the samples</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Fraud</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="c1">#training instance that are fraudulent
</span><span class="n">Valid</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="c1">#training instances that are valid
</span>
<span class="c1">#ratio of fradulent to valid cases
</span><span class="n">Outlier_frac</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Fraud</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Valid</span><span class="p">))</span>
<span class="n">Outlier_frac</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Fraud counts:'</span><span class="p">,</span><span class="n">Fraud</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">'</span><span class="se">\n</span><span class="s">Non-fraud counts:'</span><span class="p">,</span> <span class="n">Valid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">Proportion of outliers in the dataset:'</span><span class="p">,</span> <span class="n">Outlier_frac</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fraud counts: 492
Non-fraud counts: 284315

Proportion of outliers in the dataset: 0.0017304750013189597
</code></pre></div></div>

<h3 id="instantiates-different-classes-for-training">Instantiates different classes for training</h3>

<p>Each classification algorithm has its inherent biases, and no single classification model enjoys superiority if we donâ€™t make any assumptions about the task.  It is therefore expedient to compare at least a handful of different algorithms in order to train and select the best performing model - thatâ€™s what we are going to do here.  All selected models will be trained on the training data, the learning algorithm searched for the model parameter values that minimize a cost function.</p>

<p><code class="highlighter-rouge">Let's first consider unsupervised learning algorithms:</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Instantiate IsolationForest class
</span><span class="n">IF_clf</span> <span class="o">=</span> <span class="n">IsolationForest</span><span class="p">(</span><span class="n">max_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">),</span> <span class="n">contamination</span> <span class="o">=</span> <span class="n">Outlier_frac</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="c1">#fit and predicting labels
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">IF_clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)</span>

<span class="c1">#reshape the prediction value to 0 for valid, 1 for fraud
</span><span class="n">y_pred</span><span class="p">[</span><span class="n">y_pred</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y_pred</span><span class="p">[</span><span class="n">y_pred</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#Instantiate LocalOutlierFactor class
</span><span class="n">LOF_clf</span> <span class="o">=</span> <span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">contamination</span> <span class="o">=</span> <span class="n">Outlier_frac</span><span class="p">)</span>

<span class="c1">#fit and predicting labels
</span><span class="n">y_pred2</span> <span class="o">=</span> <span class="n">LOF_clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">)</span>

<span class="n">y_pred2</span><span class="p">[</span><span class="n">y_pred2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y_pred2</span><span class="p">[</span><span class="n">y_pred2</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div>

<p><strong>Note</strong>:
Unsupervised Pretraining, both models are more computationally expensive!!!</p>

<p><code class="highlighter-rouge">Additional: let's look at this powerful supervised learning algorithm</code></p>

<p>A C-Support Vector Classification, is part of Support Vector Machines (SVMs), is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection.  This Machine Learning algorithm is computationally expensive to compute all the additional features, especially when training sets is large.</p>

<p>Parameters to adjust:</p>

<ul>
  <li>
    <p>The optimal value of the C parameter will depend on your dataset, and should be tuned via cross-validation.  If your SVC model is overfitting, we can always try regularizing it by reducing C.</p>
  </li>
  <li>
    <p>Since the training set is not too large, we shall try the Gaussian Radial Basis Function (RBF) kernel; it works well in most cases.</p>
  </li>
  <li>
    <p>So $\gamma$ acts like a regularization hyperparameter: if the model is overfitting, it must be reduced, and if it is underfitting, it must be increased (similar to the C hyperparameter).</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span> <span class="c1">#C-Support Vector Classification
</span>
<span class="n">svc_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span> <span class="o">=</span> <span class="s">'rbf'</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="s">'auto'</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span> <span class="c1">#instantiate C-Support Vector class
</span>

<span class="n">svc_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span> <span class="c1">#training the model
</span></code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVC(C=8, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred3</span> <span class="o">=</span> <span class="n">svc_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">)</span>

</code></pre></div></div>

<h1 id="4-model-evaluation---selecting-optimal-predictive-model">4. Model Evaluation - selecting optimal predictive model</h1>

<p>We shall now show a <code class="highlighter-rouge">confusion matrix</code> showing the frequency of misclassifications by our
classifier. to measure performance, <code class="highlighter-rouge">classification accuracy</code> which is defined as the proportion of correctly classified instances will be used, to see the performance of each classifier.  In this part, the <code class="highlighter-rouge">test set</code> will be used to evaluate each model performance</p>

<h3 id="accuracy-score">Accuracy score</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred_IF</span> <span class="o">=</span> <span class="n">IF_clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">)</span>

<span class="n">y_pred_IF</span><span class="p">[</span><span class="n">y_pred_IF</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y_pred_IF</span><span class="p">[</span><span class="n">y_pred_IF</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>


<span class="n">y_pred_LOF</span> <span class="o">=</span> <span class="n">LOF_clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">)</span>

<span class="n">y_pred_LOF</span><span class="p">[</span><span class="n">y_pred_LOF</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">y_pred_LOF</span><span class="p">[</span><span class="n">y_pred_LOF</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>


<span class="sb">``</span><span class="err">`</span><span class="n">python</span>

</code></pre></div></div>

<p>print(â€˜\n\nâ€”â€”â€”â€”â€”â€”â€“Isolation Forestâ€”â€”â€”â€”â€”â€”-â€˜)
#print(â€˜score prediction: {:.2f}â€™ .format(score_pred))
print(â€˜\n Accuracy score (training set): â€˜, accuracy_score(Y_train, y_pred))
print(â€˜\n Accuracy score (test set): â€˜, accuracy_score(Y_val, y_pred_IF))
print(â€˜\n Number of error :â€™, (Y_val != y_pred_IF).sum())</p>

<p>print(â€˜\n\nâ€”â€”â€”â€”â€”â€”â€“LocalOutlierFactorâ€”â€”â€”â€”â€”â€”-â€˜)
#print(â€˜score prediction: {:.2f}â€™ .format(score_pred))
print(â€˜\n Accuracy score (training set): â€˜, accuracy_score(Y_train, y_pred2))
print(â€˜\n Accuracy score (test set): â€˜, accuracy_score(Y_val, y_pred_LOF))
print(â€˜\n Number of error :â€™, (Y_val != y_pred_LOF).sum())</p>

<p>print(â€˜\n\nâ€”â€”â€”â€”â€”â€”â€“Support Vector Classâ€”â€”â€”â€”â€”â€”-â€˜)
print(â€˜\n Accuracy score (training set): â€˜, accuracy_score(Y_train, svc_clf.predict(X_train_std)))
print(â€˜\n Accuracy score (validation set): â€˜, accuracy_score(Y_val, y_pred3))
print(â€˜\n Number of error :â€™, (Y_val != y_pred3).sum())</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
    --------------------Isolation Forest-------------------

     Accuracy score (training set):  0.9976694682788738

     Accuracy score (test set):  0.997559776693234

     Number of error : 139


    --------------------LocalOutlierFactor-------------------

     Accuracy score (training set):  0.9965371195330158

     Accuracy score (test set):  0.9966117762719006

     Number of error : 193


    --------------------Support Vector Class-------------------


We know that accuracy works well on balanced datasets.  The dataset is highly imbalanced, so we cannot use accuracy to quantify model performance. So we need another perfomance measure for imbalanced datasets.  We shall consider using `classificication accuracy` to quantify the perfomance of each model.

### Confusion matrix

Some terms used in a confusion matrix are:

- True positives (TPs): True positives are cases when we predict the transactions as fraudulent when the transaction actually is fraudulent.

- True negatives (TNs): Cases when we predict transections as non-fradulent when transactions actually are non-fraudulant.

- False positives (FPs): When we predict the transactions as non-fraudulent  when the transactions are actually fraudulent. FPs are also considered to be type I errors.

- False negatives (FNs): When we predict the transactions as fraudulent when the transactions actually are non-fraudulent. FNs are also considered to be type II errors.


```python
names = ['Non-fradulent', 'Fradulent']

mat1 = confusion_matrix(Y_val, y_pred_IF)

plt.figure(figsize = (15,10))
plt.subplot(2, 2, 1)
sns.set(font_scale = 1.2)
sns.heatmap(mat1, cbar = True, square = True, annot = True, yticklabels = names,
            annot_kws={'size': 15}, xticklabels = names, cmap = 'RdPu')
plt.title('Isolation Forest model')
plt.xlabel('Predicted value')
plt.ylabel('True value')
plt.tight_layout()


mat2 = confusion_matrix(Y_val, y_pred_LOF)

plt.subplot(2, 2, 2)
sns.set(font_scale = 1.2)
sns.heatmap(mat2, cbar = True, square = True, annot = True, yticklabels = names,
            annot_kws={'size': 15}, xticklabels = names, cmap = 'RdPu')
plt.title('LocalOutlierFactor model')
plt.xlabel('Predicted value')
plt.ylabel('True value')
plt.tight_layout()

mat3 = confusion_matrix(Y_val, y_pred3)

plt.subplot(2, 2, 3)
sns.set(font_scale = 1.2)
sns.heatmap(mat3, cbar = True, square = True, annot = True, yticklabels = names,
            annot_kws={'size': 15}, xticklabels = names, cmap = 'RdPu')
plt.title('C-Support Vector model')
plt.xlabel('Predicted value')
plt.ylabel('True value')
plt.tight_layout()
plt.show()
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1DT0_WUD29rWrhLswtDAjnsYzZJsg9N66" alt="png" /></p>

<h3 id="classification-report">Classification report</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> ---------------------Isolation Forest model--------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">y_pred_IF</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> --------------------LocalOutlierFactor model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">y_pred_LOF</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------C-Support Vector model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">y_pred3</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ---------------------Isolation Forest model--------------------

               precision    recall  f1-score   support

Non-fradulent       1.00      1.00      1.00     56864
    Fradulent       0.29      0.30      0.29        98

    micro avg       1.00      1.00      1.00     56962
    macro avg       0.65      0.65      0.65     56962
 weighted avg       1.00      1.00      1.00     56962



 --------------------LocalOutlierFactor model-------------------

               precision    recall  f1-score   support

Non-fradulent       1.00      1.00      1.00     56864
    Fradulent       0.02      0.02      0.02        98

    micro avg       1.00      1.00      1.00     56962
    macro avg       0.51      0.51      0.51     56962
 weighted avg       1.00      1.00      1.00     56962



 -----------------------C-Support Vector model-------------------

               precision    recall  f1-score   support

Non-fradulent       1.00      1.00      1.00     56864
    Fradulent       0.95      0.72      0.82        98

    micro avg       1.00      1.00      1.00     56962
    macro avg       0.97      0.86      0.91     56962
 weighted avg       1.00      1.00      1.00     56962
</code></pre></div></div>

<p><code class="highlighter-rouge">LocalOutlierFactor Model</code>: For class 0 (Non-fraudulent transaction), we have precision of ~ 100% and for class 1 (Fraudulent transaction), we have 0.2% - thatâ€™s not good.  Precision counts for false-positives and recall counts for false-negatives.</p>

<p><code class="highlighter-rouge">Isolation Forest Model</code>: We have a precision of ~ 30%, thatâ€™s better than ~ 2% obtained in <code class="highlighter-rouge">LocalOutlierFactor Model</code>.  But, still, we are only correctly identifying 30% of transactions that are actual fraudulent cases.  We do also have a lot of false-positives (this could frustrate customer with false alarm), and we also have a recall (false-Negative) of ~ 30%</p>

<p><code class="highlighter-rouge">C-Support Vector Model</code>: We have achieved a precision of ~ 95%, thatâ€™s better than ~ 30% obtained in <code class="highlighter-rouge">Isolation Forest Model</code>.  But, still, we are only correctly identifying ~ 82% of transactions that are actual fraudulent cases - thatâ€™s much better.</p>

<h1 id="conclusion">Conclusion</h1>

<p>From unsupervised models and the <code class="highlighter-rouge">f1-score</code>, <code class="highlighter-rouge">IsolationForest</code> model performs better than <code class="highlighter-rouge">LocalOutlierFactor</code> model. The dataset used in this work was sufficiently complex, so random forest method was able to produce better results.  So 30% of the time we are going to detect fraudulent transaction, unless the culprit made 4 transactions statistically, we will find them everytime! However, <code class="highlighter-rouge">C-Support Vector</code> model, which is supervised performs, was able to produce best results!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET