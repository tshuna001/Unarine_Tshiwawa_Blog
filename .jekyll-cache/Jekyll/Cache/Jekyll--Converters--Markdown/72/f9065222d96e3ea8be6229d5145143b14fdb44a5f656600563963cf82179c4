I"›<script type="math/tex; mode=display">\mathrm{\textbf{Diabetes Task}}</script>

<p>The objective in this work is to use information from the patient to predict whether or not the patient has diabetes.  There are 8 features (predictor variables) and 1 label (response variable). The data is collected from acutal patients and represents a task which might commonly be undertaken by a human doctor interested in identifying the patients most at risk for diabetes in order to recommend preventative measures. The data was originally collected by the National Institute of Diabetes and Digestive and Kidney Diseases from a set of females at least 21 years old and of Pima Indian Heritage.</p>

<p>Clearly, we see that this is a machine leaning - binary classification task - type of a problem where the machine learning algorithm learns a set of rules in order to distinguish between two possible classes: non-diabetic and diabetic conditions in different petients. The main goal in supervised learning is to learn a model (based on past observations) from labeled training data that allows us to make predictions about unseen or future data of new instances.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Import libraries
</span><span class="s">'''Main'''</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">gzip</span>

<span class="s">'''Data Viz'''</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">'notebook'</span><span class="p">)</span>
<span class="n">color</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Javascript</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_columns'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_rows'</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="data">Data</h2>

<p>data source: <a href="https://www.kaggle.com/uciml/pima-indians-diabetes-database/data">diabetes</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">name</span> <span class="o">=</span> <span class="s">'datasets_228_482_diabetes.csv'</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<p>Features of each instance in the dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
      dtype='object')
</code></pre></div></div>

<p>Dimension of the dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(768, 9)
</code></pre></div></div>

<p>We are dealing with small dataset according to machine learning standard, however we shall deal with this problem at a later stage.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<p>Dataset information:</p>

<p>The info() method is useful to get a quick description of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
Pregnancies                 768 non-null int64
Glucose                     768 non-null int64
BloodPressure               768 non-null int64
SkinThickness               768 non-null int64
Insulin                     768 non-null int64
BMI                         768 non-null float64
DiabetesPedigreeFunction    768 non-null float64
Age                         768 non-null int64
Outcome                     768 non-null int64
dtypes: float64(2), int64(7)
memory usage: 54.1 KB



None
</code></pre></div></div>

<h3 id="basic-statistics">Basic statistics:</h3>

<p>Summary of each numerical attribute in dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.845052</td>
      <td>120.894531</td>
      <td>69.105469</td>
      <td>20.536458</td>
      <td>79.799479</td>
      <td>31.992578</td>
      <td>0.471876</td>
      <td>33.240885</td>
      <td>0.348958</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.369578</td>
      <td>31.972618</td>
      <td>19.355807</td>
      <td>15.952218</td>
      <td>115.244002</td>
      <td>7.884160</td>
      <td>0.331329</td>
      <td>11.760232</td>
      <td>0.476951</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.078000</td>
      <td>21.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>99.000000</td>
      <td>62.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>27.300000</td>
      <td>0.243750</td>
      <td>24.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>117.000000</td>
      <td>72.000000</td>
      <td>23.000000</td>
      <td>30.500000</td>
      <td>32.000000</td>
      <td>0.372500</td>
      <td>29.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.000000</td>
      <td>140.250000</td>
      <td>80.000000</td>
      <td>32.000000</td>
      <td>127.250000</td>
      <td>36.600000</td>
      <td>0.626250</td>
      <td>41.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>17.000000</td>
      <td>199.000000</td>
      <td>122.000000</td>
      <td>99.000000</td>
      <td>846.000000</td>
      <td>67.100000</td>
      <td>2.420000</td>
      <td>81.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>The count, mean, min, and max rows are self-explanatory.  Note that the null values are ignored, but we do not have to worry about that in this case â€“ both datasets have no null values. The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations falls</p>

<p>So letâ€™s look at the following attribute/features indicated above, minimum <code class="highlighter-rouge">glucose</code>, <code class="highlighter-rouge">blood pressure</code>, <code class="highlighter-rouge">skin thickness</code>, <code class="highlighter-rouge">insulin</code>, and <code class="highlighter-rouge">BMI</code> are all 0. This appears suspect because these are physical quantities that cannot be 0 for a live person! So we shall assume that all 0 in the above-mentioned features are as a result of lack of data.  To fill in these missing values, we will replace them with the median value in the column. There are other, more complicated methods for filling in missing values, but in practice, median imputation generally performs well.</p>

<h2 id="imputing-missing-values">Imputing missing values</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="p">[</span><span class="s">'Glucose'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Glucose'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Glucose'</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()})</span>
<span class="n">df1</span><span class="p">[</span><span class="s">'BloodPressure'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'BloodPressure'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">df1</span><span class="p">[</span><span class="s">'BloodPressure'</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()})</span>
<span class="n">df1</span><span class="p">[</span><span class="s">'SkinThickness'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'SkinThickness'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">df1</span><span class="p">[</span><span class="s">'SkinThickness'</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()})</span>
<span class="n">df1</span><span class="p">[</span><span class="s">'Insulin'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Insulin'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Insulin'</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()})</span>
<span class="n">df1</span><span class="p">[</span><span class="s">'BMI'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'BMI'</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="n">df1</span><span class="p">[</span><span class="s">'BMI'</span><span class="p">]</span><span class="o">.</span><span class="n">median</span><span class="p">()})</span>
</code></pre></div></div>

<p>Letâ€™s verify:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
      <td>768.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>3.845052</td>
      <td>121.656250</td>
      <td>72.386719</td>
      <td>27.334635</td>
      <td>94.652344</td>
      <td>32.450911</td>
      <td>0.471876</td>
      <td>33.240885</td>
      <td>0.348958</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.369578</td>
      <td>30.438286</td>
      <td>12.096642</td>
      <td>9.229014</td>
      <td>105.547598</td>
      <td>6.875366</td>
      <td>0.331329</td>
      <td>11.760232</td>
      <td>0.476951</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>44.000000</td>
      <td>24.000000</td>
      <td>7.000000</td>
      <td>14.000000</td>
      <td>18.200000</td>
      <td>0.078000</td>
      <td>21.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>1.000000</td>
      <td>99.750000</td>
      <td>64.000000</td>
      <td>23.000000</td>
      <td>30.500000</td>
      <td>27.500000</td>
      <td>0.243750</td>
      <td>24.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>3.000000</td>
      <td>117.000000</td>
      <td>72.000000</td>
      <td>23.000000</td>
      <td>31.250000</td>
      <td>32.000000</td>
      <td>0.372500</td>
      <td>29.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>6.000000</td>
      <td>140.250000</td>
      <td>80.000000</td>
      <td>32.000000</td>
      <td>127.250000</td>
      <td>36.600000</td>
      <td>0.626250</td>
      <td>41.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>17.000000</td>
      <td>199.000000</td>
      <td>122.000000</td>
      <td>99.000000</td>
      <td>846.000000</td>
      <td>67.100000</td>
      <td>2.420000</td>
      <td>81.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>Great! Now we are good to continue :)</p>

<h3 id="impute-missing-values">Impute missing values</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pregnancies                 0
Glucose                     0
BloodPressure               0
SkinThickness               0
Insulin                     0
BMI                         0
DiabetesPedigreeFunction    0
Age                         0
Outcome                     0
dtype: int64
</code></pre></div></div>

<h3 id="looking-for-correlations">Looking for Correlations:</h3>

<p>Since the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearsonâ€™s r) between every pair of attributes using the corr() method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="n">corr_matrix</span><span class="p">[</span><span class="s">"Outcome"</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Outcome                     1.000000
Glucose                     0.492782
BMI                         0.312249
Age                         0.238356
Pregnancies                 0.221898
SkinThickness               0.189065
DiabetesPedigreeFunction    0.173844
BloodPressure               0.165723
Insulin                     0.148457
Name: Outcome, dtype: float64
</code></pre></div></div>

<p>Note: the correlation coefficient ranges from â€“1 to 1. When it is close to 1, it means that there is a strong positive correlation.  When the coefficient is close to â€“1, it means that there is a strong negative correlation.  Finally, coefficients close to zero mean that there is no linear correlation</p>

<p>Glucose has the highest correlation value with the outcome, however none of the features are strongly correlated with the outcome and there are no negative correlations.</p>

<h1 id="1-exploratory-data-analasis">1. Exploratory Data Analasis</h1>

<p>Moving on, we have few more numeric columns to explore, so we can use the <code class="highlighter-rouge">hist function</code> provided within pandas to save time.</p>

<p>A histogram for each numerical attribute</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1-hjihmait1Vgf3fCEGSYQ3yBSd6OR6HU" alt="png" /></p>

<p>Letâ€™s analyse the result:</p>

<p>We can see that the feature <code class="highlighter-rouge">Outcome</code> is actually binary categorical features: It represent two possible values similar to gender: Male or Female. Therefore, this is actually categorical feature but already encoded as numeric column.</p>

<p>Mapping numerical feature (<code class="highlighter-rouge">Outcome</code>) to categorical feature.  O and 1 indicates patients that are non-diabetic and diabetic, respectively.  We are doing this bacause we want to clearly visualise numerical features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Outcome'</span><span class="p">]</span><span class="o">.</span><span class="nb">map</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span><span class="s">'Diabetic'</span><span class="p">,</span> <span class="mi">0</span><span class="p">:</span><span class="s">'Non-diabetic'</span><span class="p">})</span>
<span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array(['Diabetic', 'Non-diabetic'], dtype=object)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h2 id="data-viz-pie-plot">data viz: pie plot</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span> <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                         <span class="n">shadow</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span>

<span class="n">df1</span><span class="p">[</span><span class="s">'Class'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Non-diabetic    500
Diabetic        268
Name: Class, dtype: int64
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1BRE3ErNBom2TftY423wLRTbVivPb0P4o" alt="png" /></p>

<p>Note: <code class="highlighter-rouge">We see that we have huge disparity between diabetic and non-diabetic patience.</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="impute-missing-values-1">Impute Missing Values</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pregnancies                 0
Glucose                     0
BloodPressure               0
SkinThickness               0
Insulin                     0
BMI                         0
DiabetesPedigreeFunction    0
Age                         0
Outcome                     0
Class                       0
dtype: int64
</code></pre></div></div>

<p>There are no missing values in the dataset.</p>

<h2 id="check-correlation-of-features">Check correlation of features</h2>

<p>The correlation coefficient only measures linear correlations.  Letâ€™s check for linearity amoung features.  So we can easily compute the standard correlation coefficient (also called Pearsonâ€™s r) between every pair of attributes using the corr() method:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="n">corr_matrix</span><span class="p">[</span><span class="s">'Outcome'</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">corr_mat</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1rh94IkHFRkRsYRJO0_cQnO--zY9AV072" alt="png" /></p>

<p>The correlation coefficient ranges from â€“1 to 1. When it is close to 1, it means that there is a strong positive correlation.  When the coefficient is close to â€“1, it means that there is a strong negative correlation.  Finally, coefficients close to zero mean that there is no linear correlation.</p>

<p>Info: We see noticeable correlation between, <code class="highlighter-rouge">Glucose</code> and <code class="highlighter-rouge">Outcome</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h1 id="2-preprocessing---getting-data-into-shape">2. Preprocessing - getting data into shape</h1>

<p>To determine whether our machine learning algorithm not only performs well on the training set but also generalizes well to new data, we also want to randomly divide the dataset into a separate <code class="highlighter-rouge">training</code> and <code class="highlighter-rouge">validation set</code>. <code class="highlighter-rouge">We use the training set to train and optimize our machine learning model</code>, while we keep the <code class="highlighter-rouge">tvalidation set until the very end to evaluate the final model</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#feature matrix
</span><span class="n">x</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'Class'</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1">#Target vector/labels
</span><span class="n">y</span> <span class="o">=</span> <span class="n">df1</span><span class="p">[</span><span class="s">'Outcome'</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(768, 9)
(768,)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'---------------------------------Feature Matrix--------------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'----------------------------------Target Vector--------------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------Feature Matrix--------------------------
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148.0</td>
      <td>72.0</td>
      <td>35.0</td>
      <td>30.5</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85.0</td>
      <td>66.0</td>
      <td>29.0</td>
      <td>30.5</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183.0</td>
      <td>64.0</td>
      <td>23.0</td>
      <td>30.5</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89.0</td>
      <td>66.0</td>
      <td>23.0</td>
      <td>94.0</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137.0</td>
      <td>40.0</td>
      <td>35.0</td>
      <td>168.0</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>116.0</td>
      <td>74.0</td>
      <td>23.0</td>
      <td>30.5</td>
      <td>25.6</td>
      <td>0.201</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>78.0</td>
      <td>50.0</td>
      <td>32.0</td>
      <td>88.0</td>
      <td>31.0</td>
      <td>0.248</td>
      <td>26</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>115.0</td>
      <td>72.0</td>
      <td>23.0</td>
      <td>30.5</td>
      <td>35.3</td>
      <td>0.134</td>
      <td>29</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
      <td>197.0</td>
      <td>70.0</td>
      <td>45.0</td>
      <td>543.0</td>
      <td>30.5</td>
      <td>0.158</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>8</td>
      <td>125.0</td>
      <td>96.0</td>
      <td>23.0</td>
      <td>30.5</td>
      <td>32.0</td>
      <td>0.232</td>
      <td>54</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>----------------------------------Target Vector--------------------------




0    1
1    0
2    1
3    0
4    1
5    0
6    1
7    0
8    1
9    1
Name: Outcome, dtype: int64
</code></pre></div></div>

<h3 id="data-splicing">Data Splicing</h3>

<p>We are going to split the data into three sets -  80%, 20% for training, validation, respectively.  This means, 80% of the data will be set to train and optimize our machine learning model and 20% will be used to validate the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>Letâ€™s verify data partitioning:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="p">[</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">]:</span>

    <span class="k">print</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">*</span><span class="mi">100</span><span class="p">),</span> <span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Done!!&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>80 %
20 %
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; Done!!&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
</code></pre></div></div>

<h3 id="feature-scaling">Feature scaling</h3>

<p><code class="highlighter-rouge">Support vector machines</code> is sensitive to the feature scales.  Data are not usually presented to the machine learning algorithm in exactly the same raw form as it is found. Usually data are scaled to a specific range in a process called normalization for optimal performance.</p>

<p>We are going to make selected features to the same scale for optimal performance, which is often achieved by transforming the features in the range [0, 1]: standardize features by removing the mean and scaling to unit variance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="n">X_train_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_val_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h1 id="3-machine-learning-model">3. Machine Learning model</h1>

<p>Now, the feature matrix and labels are ready for training. This is the part where different classifier will be instantiated (for training) and a model which performs better in both training and test set will be used for further predictions.</p>

<p>It is important to note that the parameters for the previously mentioned procedures, such as feature scaling are solely obtained from the training dataset, and the same parameters are later reapplied to transform the test dataset, as well as any new data samplesâ€”the performance measured on the test data may be overly optimistic otherwise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>


<span class="c1">#Logistic regression
</span><span class="k">print</span><span class="p">(</span><span class="s">'-----------------------Logistic Regression Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">LR</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">LR</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">LR</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">LR</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>


<span class="c1">#Decision Tree
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">DT</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">DT</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------Decision Tree Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">DT</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">DT</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>


<span class="c1">#Random forest
</span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1">#instantiate random forest class
</span><span class="n">RF</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="n">RF</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------Random Forest Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">RF</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">RF</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>



<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="c1">#KNN = Pipeline(steps=[('preprocessor', preprocessing.StandardScaler()),
#                     ('model', KNeighborsClassifier())])
</span><span class="n">KNN</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">KNN</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">---------------------k-Nearest Neighbor Classifier------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">KNN</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">KNN</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">---------------------C-Support Vector Classifier------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>


<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span> <span class="c1">#C-Support Vector Classification
</span>
<span class="n">svc_clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span> <span class="c1">#instantiate C-Support Vector class
</span>
<span class="n">svc_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span> <span class="c1">#training the model
</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">svc_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">svc_clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-------------------Gaussian Naive Bayes classifier------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gnb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuraccy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">gnb</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)))</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----------------------Logistic Regression Classifier---------------------

Accuraccy in training set: 1.00000
Accuraccy in validation set: 1.00000

-----------------------Decision Tree Classifier---------------------

Accuraccy in training set: 1.00000
Accuraccy in validation set: 1.00000

-----------------------Random Forest Classifier---------------------

Accuraccy in training set: 1.00000
Accuraccy in validation set: 1.00000

---------------------K-Nearest Neighbor Classifier------------------

Accuraccy in training set: 0.51140
Accuraccy in validation set: 0.50000

---------------------C-Support Vector Classifier------------------

Accuraccy in training set: 1.00000
Accuraccy in validation set: 1.00000

-------------------Gaussian Naive Bayes classifier------------------

Accuraccy in training set: 1.00000
Accuraccy in validation set: 1.00000
</code></pre></div></div>

<p>We know that accuracy works well on balanced data.  The data is imbalanced, so we cannot use accuracy to quantify model performance. So we need another perfomance measure for imbalanced data.  We shall consider using <code class="highlighter-rouge">Confusion matrix</code>, <code class="highlighter-rouge">F1 score metric</code>, and <code class="highlighter-rouge">Receiver Operating Characteristics (ROC) Curve</code> to quantify the perfomance.</p>

<h1 id="4-model-evaluation---selecting-optimal-predictive-model">4. Model Evaluation - selecting optimal predictive model</h1>

<p>We shall now show a <code class="highlighter-rouge">confusion matrix</code>, showing the frequency of misclassifications by our
classifier, to measure performance, <code class="highlighter-rouge">classification accuracy</code> which is defined as the proportion of correctly classified instances will be used, to see the performance of each classifier.  In this part, the <code class="highlighter-rouge">test set</code> will be used to evaluate each model performance</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Non-diabetic'</span><span class="p">,</span> <span class="s">'Diabetic'</span><span class="p">]</span>

<span class="n">pred1</span> <span class="o">=</span> <span class="n">LR</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mat1</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat1</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Logistic Regression Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">pred2</span> <span class="o">=</span> <span class="n">DT</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mat2</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat2</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">pred3</span> <span class="o">=</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mat3</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred3</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat3</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Random Forest Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">pred4</span> <span class="o">=</span> <span class="n">KNN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mat4</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat4</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'K-Nearest Neighbor Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">pred5</span> <span class="o">=</span> <span class="n">svc_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mat5</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat5</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'C-Support Vector Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">pred6</span> <span class="o">=</span> <span class="n">gnb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mat6</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred6</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat6</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Gaussian Naive Bayes Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1RJsm9twgko-gfkzExOSSgmj4g3yQ8Fc6" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="classification-report">Classification report</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> ---------------------Logistic Regression model--------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred1</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> --------------------Decision Tree model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred2</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------Random Forest model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred3</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> --------------------K-Nearest Neighbor model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred4</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------C-Support Vector model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred5</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------Gaussian Naive Bayes model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred6</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ---------------------Logistic Regression model--------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 --------------------Decision Tree model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 -----------------------Random Forest model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 --------------------K-Nearest Neighbor model-------------------

              precision    recall  f1-score   support

Non-diabetic       0.66      0.47      0.55       100
    Diabetic       0.36      0.56      0.44        54

   micro avg       0.50      0.50      0.50       154
   macro avg       0.51      0.51      0.49       154
weighted avg       0.56      0.50      0.51       154



 -----------------------C-Support Vector model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 -----------------------Gaussian Naive Bayes model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154
</code></pre></div></div>

<h2 id="receiver-operating-characteriscics-roc-curve">Receiver Operating Characteriscics (ROC) Curve</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s">'--'</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'r'</span><span class="p">)</span>                                    
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'False Positive Rate (Fall-Out)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> <span class="c1"># Not shown
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True Positive Rate (Recall)'</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>    <span class="c1"># Not shown
</span>    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>   
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">roc_curve</span>


<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs1</span> <span class="o">=</span> <span class="n">LR</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc1</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'bmh'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot the roc curve
</span><span class="n">fpr1</span><span class="p">,</span> <span class="n">tpr1</span><span class="p">,</span> <span class="n">thresholds1</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs1</span><span class="p">)</span>

<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr1</span><span class="p">,</span> <span class="n">tpr1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Logistic Regression - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc1</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>

<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs2</span> <span class="o">=</span> <span class="n">DT</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc2</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs2</span><span class="p">)</span>
<span class="n">fpr2</span><span class="p">,</span> <span class="n">tpr2</span><span class="p">,</span> <span class="n">thresholds2</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs2</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># Plot the roc curve
</span><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr2</span><span class="p">,</span> <span class="n">tpr2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc2</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>


<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs3</span> <span class="o">=</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc3</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs3</span><span class="p">)</span>

<span class="n">fpr3</span><span class="p">,</span> <span class="n">tpr3</span><span class="p">,</span> <span class="n">thresholds3</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs3</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr3</span><span class="p">,</span> <span class="n">tpr3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Random Forest - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc3</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>

<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs4</span> <span class="o">=</span> <span class="n">KNN</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc4</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs4</span><span class="p">)</span>

<span class="n">fpr4</span><span class="p">,</span> <span class="n">tpr4</span><span class="p">,</span> <span class="n">thresholds4</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs4</span><span class="p">)</span>

<span class="c1"># Plot the roc curve
</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr4</span><span class="p">,</span> <span class="n">tpr4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'K-Nearest Neighbor - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc4</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'K-Nearest Neighbor - ROC Curve, \n AUC = 0.5128')
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1wQl5F4-tY_lqCZjQpAMkwwGUWf5Q5gx7" alt="png" /></p>

<p>Point to note: K-Nearest Neighbor (KNN) Classiffier performed pooly.  So we shall subject all models to parameter tuning to further evaluate them.  We want to ensure that we get a model which doesnâ€™t overfit or underfit.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="fine-tune-models">Fine-Tune Models</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

</code></pre></div></div>

<h4 id="randomized-search">Randomized search</h4>

<p>We shall use <code class="highlighter-rouge">Randomized Search</code>.  <code class="highlighter-rouge">RandomizedSearchCV</code> class can be used in much the same way as the <code class="highlighter-rouge">GridSearchCV</code> class, but instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration.</p>

<p>Note:  We shall use 5-fold cross-validation mechanisms to resample the data to produce multiple datasets, that is, for a given hyperparameter setting, each of the 5 folds takes turns being the hold-out validation set; a model is trained on the rest of the 5 â€“ 1 folds and measured on the held-out fold.  The purpose performing cross-validation is because we have a <code class="highlighter-rouge">samll dataset</code> by machine learning standard</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[1m"</span><span class="o">+</span><span class="s">'Decision Tree Classifier params:'</span><span class="o">+</span><span class="s">"</span><span class="se">\033</span><span class="s">[10m"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">DT</span><span class="o">.</span><span class="n">get_params</span><span class="p">)</span>


<span class="n">params11</span> <span class="o">=</span> <span class="p">{</span><span class="s">"max_depth"</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span> <span class="s">"min_samples_leaf"</span><span class="p">:</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)],</span>
               <span class="s">"criterion"</span><span class="p">:</span> <span class="p">[</span><span class="s">"gini"</span><span class="p">,</span><span class="s">"entropy"</span><span class="p">],</span> <span class="s">"max_features"</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span> <span class="p">}</span>


<span class="n">DT_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">DT</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">params11</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">DT_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------Decision Tree Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">DT</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">DT</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">DT_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">DT_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1mDecision Tree Classifier params:[10m



&lt;bound method BaseEstimator.get_params of DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=42,
            splitter='best')&gt;


Fitting 5 folds for each of 18 candidates, totalling 90 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.9s



-----------------------Decision Tree Classifier---------------------

Accuracy in training set: 1.00000
Accuracy in validation set: 1.00000
Accuracy in training set(hyperparameter tunning): 1.00000
Accuracy in validation set(hyperparameter tunning): 1.00000


[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    9.6s finished
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#parameter settings
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\033</span><span class="s">[1m"</span><span class="o">+</span><span class="s">'Random Forest Classifier params:'</span><span class="o">+</span><span class="s">"</span><span class="se">\033</span><span class="s">[10m"</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">RF</span><span class="o">.</span><span class="n">get_params</span><span class="p">)</span>


<span class="n">params22</span> <span class="o">=</span> <span class="p">{</span><span class="s">"max_depth"</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="bp">None</span><span class="p">],</span><span class="s">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="s">'max_features'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
           <span class="s">'bootstrap'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span> <span class="s">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">],</span> <span class="s">'max_features'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
           <span class="s">"criterion"</span><span class="p">:</span> <span class="p">[</span><span class="s">"gini"</span><span class="p">,</span><span class="s">"entropy"</span><span class="p">]}</span>


<span class="n">RF_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">RF</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">params22</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                               <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">RF_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------Random Forest Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">RF_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">RF_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))))</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">RF_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1mRandom Forest Classifier params:[10m



&lt;bound method BaseEstimator.get_params of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,
            oob_score=False, random_state=42, verbose=0, warm_start=False)&gt;


Fitting 5 folds for each of 100 candidates, totalling 500 fits


[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.6s
[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   10.7s
[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   23.9s



-----------------------Random Forest Classifier---------------------

Accuracy in training set: 1.00000
Accuracy in validation set: 1.00000
Accuracy in training set(hyperparameter tunning): 1.00000
Accuracy in validation set(hyperparameter tunning): 1.00000


[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:   26.6s finished





1.0
</code></pre></div></div>

<p><strong>Warning</strong>: the next cell may take time to run, depending on your hardware.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#parameter settings
</span><span class="n">params33</span> <span class="o">=</span> <span class="p">{</span><span class="s">'weights'</span><span class="p">:(</span><span class="s">'uniform'</span><span class="p">,</span> <span class="s">'distance'</span><span class="p">),</span> <span class="s">'n_neighbors'</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]}</span>


<span class="n">KNN_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">KNN</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">params33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                                    <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">KNN_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------K-Nearest Neighbor Classifier---------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">KNN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">KNN</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">KNN_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">KNN_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))))</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">KNN_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----------------------k-Nearest Neighbor Classifier---------------------

Accuracy in training set: 0.99837
Accuracy in validation set: 1.00000
Accuracy in training set(hyperparameter tunning): 1.00000
Accuracy in validation set(hyperparameter tunning): 1.00000





1.0
</code></pre></div></div>

<p>Note: there seemed to be a major improvement in kNN classifier after parameter tuning.</p>

<p><strong>Warning</strong>: the next cell may also take time to run, depending on your hardware.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#parameter settings
</span><span class="n">params44</span> <span class="o">=</span> <span class="p">{</span><span class="s">'kernel'</span><span class="p">:(</span><span class="s">'linear'</span><span class="p">,</span> <span class="s">'rbf'</span><span class="p">,</span> <span class="s">'poly'</span><span class="p">),</span> <span class="s">'C'</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mf">1E4</span><span class="p">,</span> <span class="mf">1E10</span><span class="p">],</span> <span class="s">'gamma'</span><span class="p">:[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]}</span>

<span class="n">svc_clf_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">svc_clf</span><span class="p">,</span> <span class="n">param_distributions</span><span class="o">=</span><span class="n">params44</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                                    <span class="n">cv</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">'neg_mean_squared_error'</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">svc_clf_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------Support Vector Machine Classifier---------------------'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">svc_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set: {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">svc_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in training set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_train</span><span class="p">,</span> <span class="n">svc_clf_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train_std</span><span class="p">))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Accuracy in validation set(hyperparameter tunning): {:.5f}'</span> <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">svc_clf_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))))</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">svc_clf_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val_std</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 5 folds for each of 10 candidates, totalling 50 fits


[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.



-----------------------Support Vector Machine Classifier---------------------
Accuracy in training set: 1.00000
Accuracy in validation set: 1.00000
Accuracy in training set(hyperparameter tunning): 1.00000
Accuracy in validation set(hyperparameter tunning): 1.00000


[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    0.7s finished





1.0
</code></pre></div></div>

<h3 id="confusion-matrix-after-parameter-tuning">Confusion matrix (after parameter tuning)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">pred2</span> <span class="o">=</span> <span class="n">DT_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mat22</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat22</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">pred33</span> <span class="o">=</span> <span class="n">RF_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>

<span class="n">mat33</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred33</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat33</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Random Forest Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">pred44</span> <span class="o">=</span> <span class="n">KNN_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mat44</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred44</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat44</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'K-Nearest Neighbor Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">pred55</span> <span class="o">=</span> <span class="n">svc_clf_search</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
<span class="n">mat55</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred55</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">font_scale</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">mat55</span><span class="p">,</span> <span class="n">cbar</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">square</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">annot</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">yticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span>
            <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s">'size'</span><span class="p">:</span> <span class="mi">15</span><span class="p">},</span> <span class="n">xticklabels</span> <span class="o">=</span> <span class="n">names</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'RdPu'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'C-Support Vector Classifier'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Predicted value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'True value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>



</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1ax_62EKGTrQ23IETZ5ldIluT9oi25ffn" alt="png" /></p>

<h3 id="classification-report-after-parameter-tuning">Classification report (after parameter tuning)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> --------------------Decision Tree model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred22</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------Random Forest model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred33</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> --------------------k-Nearest Neighbor model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred44</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n\n</span><span class="s"> -----------------------C-Support Vector model-------------------</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">pred55</span><span class="p">,</span> <span class="n">target_names</span> <span class="o">=</span> <span class="n">names</span><span class="p">))</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> --------------------Decision Tree model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 -----------------------Random Forest model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154



 --------------------K-Nearest Neighbor model-------------------

              precision    recall  f1-score   support

Non-diabetic       0.70      0.46      0.55       100
    Diabetic       0.39      0.63      0.48        54

   micro avg       0.52      0.52      0.52       154
   macro avg       0.54      0.54      0.52       154
weighted avg       0.59      0.52      0.53       154



 -----------------------C-Support Vector model-------------------

              precision    recall  f1-score   support

Non-diabetic       1.00      1.00      1.00       100
    Diabetic       1.00      1.00      1.00        54

   micro avg       1.00      1.00      1.00       154
   macro avg       1.00      1.00      1.00       154
weighted avg       1.00      1.00      1.00       154
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="roc-curve-after-parameter-tunning">ROC Curve (after parameter tunning)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#predicting class probabilities for input x_test
</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s">'bmh'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>

<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs22</span> <span class="o">=</span> <span class="n">DT_search</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc22</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs22</span><span class="p">)</span>
<span class="n">fpr22</span><span class="p">,</span> <span class="n">tpr22</span><span class="p">,</span> <span class="n">thresholds22</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs22</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># Plot the roc curve
</span><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr2</span><span class="p">,</span> <span class="n">tpr2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc22</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>


<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs33</span> <span class="o">=</span> <span class="n">RF</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc33</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs33</span><span class="p">)</span>

<span class="n">fpr33</span><span class="p">,</span> <span class="n">tpr33</span><span class="p">,</span> <span class="n">thresholds33</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs33</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr33</span><span class="p">,</span> <span class="n">tpr33</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Random Forest - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc33</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>

<span class="c1">#predicting class probabilities for input x_test
</span><span class="n">probs44</span> <span class="o">=</span> <span class="n">KNN_search</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_val</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1">#Calculate the area under the roc curve
</span><span class="n">auc44</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs44</span><span class="p">)</span>

<span class="n">fpr44</span><span class="p">,</span> <span class="n">tpr44</span><span class="p">,</span> <span class="n">thresholds44</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_val</span><span class="p">,</span> <span class="n">probs44</span><span class="p">)</span>

<span class="c1"># Plot the roc curve
</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">fpr44</span><span class="p">,</span> <span class="n">tpr44</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'K-Nearest Neighbor - ROC Curve, </span><span class="se">\n</span><span class="s"> AUC = </span><span class="si">%0.4</span><span class="s">f'</span> <span class="o">%</span> <span class="n">auc44</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1N_gWh0hLBIMg4TKoxvbzE5BnGG_jS0-6" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<h3 id="summary--conclusion">Summary \&amp; Conclusion</h3>

<p>In exploratory data analysis, the dataset unveil two aspects.  Firstly, imputing missing values in several columns was necessary because they were physically impossible.  Then the median imputation was employed as an effective method for filling impossible values (0s for BloodPressure).  Secondly, slightly positive correlation between features and response were established, despite the fact that they were not strong.  There were no feature to engineer was not necessary either since the number of observations (768) outnumbers the number of features (8), which dramatically reduces the chances of overfitting.</p>

<p>A model is a simplified version of the observations. The simplification are meant to discard the superfluous details that are unlikely to generalise to new instances. For example, a linear model makes assumption that the data is fundamentally linear and that the distance between instances and the straight line is just noise, which can safely be ignored.</p>

<p>Suprisingly all models performed better (~ 100%), except k-Nearest Neighbor (kNN), both in training and validation stage as indicated by diffent evaluation metric (Confusion matrix, F1 Score, and ROC curve).  However, we have performed parameter tunning in some models (Decision Tree, Random Forest, kNN, Support Vector Machine) and the results didnâ€™t change significantly.  kNN performed very poorly as compared to other models and it should not be considered for making predictions.  However, we acknowledge the fact that our dataset was small and caution was taken to account for that (i.e. k-fold cross validation was performed to resample the dataset).</p>

<p>So to choose the best model in this case, one has to consider number of factors:</p>

<p>Parametric machine learning algorithms used in this work: <code class="highlighter-rouge">Logistic Regression</code> and <code class="highlighter-rouge">Naive Bayes</code> models are very fast to learn from data.  They do not require as much training data and can work well even if the fit to the data is not perfect. Nonparametric machine learning algorithms used in this work, <code class="highlighter-rouge">k-Nearest Neighbors</code>, <code class="highlighter-rouge">Decision Trees</code>, <code class="highlighter-rouge">Random Forest</code> and <code class="highlighter-rouge">Support Vector Machines</code> requires a lot more training data to estimate the mapping function and are susceptible to overfit the training data and it is harder to explain why specific predictions are made.</p>

<p>So we can see that all models have their prons and cons.  However, in this work ensemble methods (Decision Tree &amp; Random Forest) will be prefered.  At this point, any model (except kNN) the reader may chose will perform just fine!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET