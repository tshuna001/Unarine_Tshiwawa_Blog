I"!<h3 id="1-identifying-the-problem">1. Identifying the problem:</h3>

<p>In this task, I seek to demonstrate how unsupervised learning machine leaning can be used in an unlabelled dataset.  Just like in classification, each instance gets assigned to a group. However, this is an unsupervised task.</p>

<p><code class="highlighter-rouge">In this work, the goal is to group similar instances together into clusters.</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="s">'notebook'</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<h1 id="dataset">Dataset</h1>

<p>The dataset contained different clusters of stars from some field.  I will use clustering algorithm to attempts to find distinct groups of data without reference to any labels.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'data.csv'</span><span class="p">)</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</code></pre></div></div>

<p>Let’s check the shape of the data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(18061, 160)
</code></pre></div></div>

<p>In this case, we see that we have 18061 samples and each sample in the dataset has 29 features.</p>

<p>Let’s impute missing values:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">---------------------------Impute missing values----------------------'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------Impute missing values----------------------



semester                                    0
FieldHalf                                   0
FIELD_1                                     0
Half                                        0
CHIP                                        0
RA_1                                      546
DEC_1                                     546
X_1                                       546
Y_1                                       546
MAG                                       546
MAGe                                      546
Name_1                                      0
Mean_Mag_1                                  0
RMS_1                                       0
Expected_RMS_1                              0
Alarm_2                                     0
Chi2_3                                      0
Jstet_4                                     0
Kurtosis_4                                  0
Lstet_4                                     0
LS_Period_1_5                               0
Log10_LS_Prob_1_5                           0
LS_SNR_1_5                                  0
Killharm_Mean_Mag_6                         0
Killharm_Per1_Fundamental_Sincoeff_6        0
Killharm_Per1_Fundamental_Coscoeff_6        0
Killharm_Per1_Amplitude_6                   0
Period_1_7                                  0
AOV_1_7                                     0
AOV_SNR_1_7                                 0
                                        ...  
rmagAB                                  12476
e_rmag                                  12476
chir                                    12207
warningr                                12021
rmagap                                  12242
rmagapAB                                12242
e_rmagap                                12242
snrr                                    12220
rmaglim                                 12219
PSFFWHMr                                12021
MJDr                                    12021
detIDr                                  12021
cleani                                  12021
imag                                    12707
imagAB                                  12707
e_imag                                  12707
chii                                    12222
warningi                                12021
imagap                                  12252
imagapAB                                12252
e_imagap                                12252
snri                                    12222
imaglim                                 12220
PSFFWHMi                                12021
MJDi                                    12021
detIDi                                  12021
Field_2                                 12021
Ext                                     12021
nbDist                                  12021
Separation                              12021
Length: 160, dtype: int64
</code></pre></div></div>

<p>Here we will remove redundant columns from the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">5</span><span class="p">:</span><span class="mi">34</span><span class="p">]</span>


<span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Name_1</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>...</th>
      <th>Killharm_Per1_Fundamental_Sincoeff_6</th>
      <th>Killharm_Per1_Fundamental_Coscoeff_6</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>RA2000</th>
      <th>DEC2000</th>
      <th>lspermin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>100009</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>...</td>
      <td>0.01155</td>
      <td>0.00484</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>17:50:33.0</td>
      <td>-32:30:19.8</td>
      <td>113.177390</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>100218</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>...</td>
      <td>0.00211</td>
      <td>0.00338</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>17:50:16.4</td>
      <td>-32:28:22.3</td>
      <td>92.062210</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>200510</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>...</td>
      <td>-0.26668</td>
      <td>-0.23009</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>17:50:13.3</td>
      <td>-32:17:09.3</td>
      <td>104.545224</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>100445</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>...</td>
      <td>-0.05318</td>
      <td>0.15253</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>17:51:02.1</td>
      <td>-32:29:06.5</td>
      <td>18.891792</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>100546</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>...</td>
      <td>-0.00545</td>
      <td>-0.00253</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>17:51:11.3</td>
      <td>-32:28:20.4</td>
      <td>71.308296</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>200327</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>...</td>
      <td>-0.00799</td>
      <td>-0.02875</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>17:51:04.1</td>
      <td>-32:22:06.4</td>
      <td>97.136510</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>200346</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>...</td>
      <td>-0.01641</td>
      <td>-0.05696</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>17:51:04.4</td>
      <td>-32:22:01.8</td>
      <td>92.754403</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>200687</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>...</td>
      <td>0.04113</td>
      <td>-0.03545</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>17:50:53.0</td>
      <td>-32:20:16.2</td>
      <td>31.794682</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 29 columns</p>
</div>

<p>Let’s now check the shape of the data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(18061, 29)
</code></pre></div></div>

<h4 id="removing-all-rows-containing-nans">Removing all rows containing nans:</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="nb">sum</span><span class="p">())</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RA_1                                    546
DEC_1                                   546
X_1                                     546
Y_1                                     546
MAG                                     546
MAGe                                    546
Name_1                                    0
Mean_Mag_1                                0
RMS_1                                     0
Expected_RMS_1                            0
Alarm_2                                   0
Chi2_3                                    0
Jstet_4                                   0
Kurtosis_4                                0
Lstet_4                                   0
LS_Period_1_5                             0
Log10_LS_Prob_1_5                         0
LS_SNR_1_5                                0
Killharm_Mean_Mag_6                       0
Killharm_Per1_Fundamental_Sincoeff_6      0
Killharm_Per1_Fundamental_Coscoeff_6      0
Killharm_Per1_Amplitude_6                 0
Period_1_7                                0
AOV_1_7                                   0
AOV_SNR_1_7                               0
AOV_NEG_LN_FAP_1_7                        0
RA2000                                  546
DEC2000                                 546
lspermin                                  0
dtype: int64
</code></pre></div></div>

<p>Let’s drop all rows/samples containing nans:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">df1</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(17515, 29)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Name_1</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>...</th>
      <th>Killharm_Per1_Fundamental_Sincoeff_6</th>
      <th>Killharm_Per1_Fundamental_Coscoeff_6</th>
      <th>Killharm_Per1_Amplitude_6</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>RA2000</th>
      <th>DEC2000</th>
      <th>lspermin</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>100009</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>...</td>
      <td>0.01155</td>
      <td>0.00484</td>
      <td>0.02505</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>17:50:33.0</td>
      <td>-32:30:19.8</td>
      <td>113.177390</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>100218</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>...</td>
      <td>0.00211</td>
      <td>0.00338</td>
      <td>0.00797</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>17:50:16.4</td>
      <td>-32:28:22.3</td>
      <td>92.062210</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>200510</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>...</td>
      <td>-0.26668</td>
      <td>-0.23009</td>
      <td>0.70445</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>17:50:13.3</td>
      <td>-32:17:09.3</td>
      <td>104.545224</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>100445</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>...</td>
      <td>-0.05318</td>
      <td>0.15253</td>
      <td>0.32307</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>17:51:02.1</td>
      <td>-32:29:06.5</td>
      <td>18.891792</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>100546</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>...</td>
      <td>-0.00545</td>
      <td>-0.00253</td>
      <td>0.01201</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>17:51:11.3</td>
      <td>-32:28:20.4</td>
      <td>71.308296</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>200327</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>...</td>
      <td>-0.00799</td>
      <td>-0.02875</td>
      <td>0.05967</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>17:51:04.1</td>
      <td>-32:22:06.4</td>
      <td>97.136510</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>200346</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>...</td>
      <td>-0.01641</td>
      <td>-0.05696</td>
      <td>0.11855</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>17:51:04.4</td>
      <td>-32:22:01.8</td>
      <td>92.754403</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>200687</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>...</td>
      <td>0.04113</td>
      <td>-0.03545</td>
      <td>0.10859</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>17:50:53.0</td>
      <td>-32:20:16.2</td>
      <td>31.794682</td>
    </tr>
    <tr>
      <th>8</th>
      <td>267.782309</td>
      <td>-32.314028</td>
      <td>1216.120</td>
      <td>3348.355</td>
      <td>17.2033</td>
      <td>0.0518</td>
      <td>200950</td>
      <td>21.85581</td>
      <td>0.59356</td>
      <td>0.12884</td>
      <td>...</td>
      <td>-0.57910</td>
      <td>-0.02658</td>
      <td>1.15941</td>
      <td>0.005993</td>
      <td>3.53978</td>
      <td>4.56843</td>
      <td>2.42907</td>
      <td>17:51:07.8</td>
      <td>-32:18:50.5</td>
      <td>110.145859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>267.835602</td>
      <td>-32.319098</td>
      <td>1976.117</td>
      <td>3264.169</td>
      <td>17.4567</td>
      <td>0.0626</td>
      <td>200985</td>
      <td>22.73157</td>
      <td>0.33644</td>
      <td>0.29747</td>
      <td>...</td>
      <td>-0.01420</td>
      <td>-0.29635</td>
      <td>0.59337</td>
      <td>0.006781</td>
      <td>4.54292</td>
      <td>6.00984</td>
      <td>3.99572</td>
      <td>17:51:20.5</td>
      <td>-32:19:08.8</td>
      <td>8.191454</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 29 columns</p>
</div>

<p>We still have three redundant columns, let’s remove them:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'RA2000'</span><span class="p">,</span><span class="s">'DEC2000'</span><span class="p">,</span> <span class="s">'Name_1'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">-----------------------------------Shape of the dataset---------------------------------'</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-----------------------------------Shape of the dataset---------------------------------



(17515, 26)
</code></pre></div></div>

<p>This dataset is now 21 dimensional: there are 21 features describing each sample.  Here we will use principal component analysis (PCA), which is a fast linear dimensionality reduction technique. It reduces the dimensions of a dataset by projecting the data onto a lower-dimensional subspace.  Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of high dimensional training set on a graph and often gain some important insight by visually detecting patterns, such as clusters.</p>

<p>For 2-dimensional subspace: The first principal component (PCA1) covers the maximum variance in the data and the second
principal component (PCA2) is orthogonal to the first principal component–all principal components are orthogonal to each other.</p>

<p>A vital part of using PCA in practice is the ability to estimate how many components are needed to describe the data.  The cumulative explained variance technique, which measures how well PCA preserves the content of the data will be employed.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">data_rescaled</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of components'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Variance (</span><span class="si">%</span><span class="s">)'</span><span class="p">)</span> <span class="c1">#for each component
</span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Cumulative explained variance'</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 1.0, 'Cumulative explained variance')
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1avTRT5wzfI3GgkUlPB1jJmNrAQ5dTgmp" alt="png" /></p>

<p>Here we see that our two-dimensional projection will lose a lot of information (as measured by the explained variance) and that we’d need about 6 components to retain about 90$\%$ of the variance.</p>

<p>The curve above quantifies how much of the total, 21-dimensional variance is contained within the first N components.</p>

<p><code class="highlighter-rouge">We will convert our dataset to two &amp; three dimension:</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">model</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span>  <span class="c1">#project from 29 to 2 dimensions
</span><span class="n">x_2D</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span> <span class="c1">#Transform the data to two dimensions
</span>
<span class="n">modell</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span> <span class="c1">#project from 29 to 3 dimensions
</span><span class="n">x_3D</span> <span class="o">=</span> <span class="n">modell</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span> <span class="c1">#Transform the data to three dimensions
</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_2D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_2D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">x</span><span class="p">[</span><span class="s">'x1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_3D</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x</span><span class="p">[</span><span class="s">'x2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_3D</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">x</span><span class="p">[</span><span class="s">'z'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_3D</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>RA_1</th>
      <th>DEC_1</th>
      <th>X_1</th>
      <th>Y_1</th>
      <th>MAG</th>
      <th>MAGe</th>
      <th>Mean_Mag_1</th>
      <th>RMS_1</th>
      <th>Expected_RMS_1</th>
      <th>Alarm_2</th>
      <th>...</th>
      <th>Period_1_7</th>
      <th>AOV_1_7</th>
      <th>AOV_SNR_1_7</th>
      <th>AOV_NEG_LN_FAP_1_7</th>
      <th>lspermin</th>
      <th>PCA1</th>
      <th>PCA2</th>
      <th>x1</th>
      <th>x2</th>
      <th>z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>267.637420</td>
      <td>-32.505489</td>
      <td>1309.394</td>
      <td>117.711</td>
      <td>11.6591</td>
      <td>0.0026</td>
      <td>15.01506</td>
      <td>0.01028</td>
      <td>0.00179</td>
      <td>6.39196</td>
      <td>...</td>
      <td>0.015565</td>
      <td>3.41553</td>
      <td>4.67124</td>
      <td>2.28794</td>
      <td>113.177390</td>
      <td>0.605842</td>
      <td>-0.486187</td>
      <td>0.605842</td>
      <td>-0.486187</td>
      <td>0.286910</td>
    </tr>
    <tr>
      <th>1</th>
      <td>267.568298</td>
      <td>-32.472861</td>
      <td>324.079</td>
      <td>665.137</td>
      <td>10.8853</td>
      <td>0.0018</td>
      <td>14.18320</td>
      <td>0.00339</td>
      <td>0.00118</td>
      <td>5.10836</td>
      <td>...</td>
      <td>0.014812</td>
      <td>5.02959</td>
      <td>6.17483</td>
      <td>4.81995</td>
      <td>92.062210</td>
      <td>0.276313</td>
      <td>-0.506676</td>
      <td>0.276313</td>
      <td>-0.506676</td>
      <td>-0.206979</td>
    </tr>
    <tr>
      <th>2</th>
      <td>267.555268</td>
      <td>-32.285918</td>
      <td>126.340</td>
      <td>3818.489</td>
      <td>16.5748</td>
      <td>0.0332</td>
      <td>20.58942</td>
      <td>0.27845</td>
      <td>0.05443</td>
      <td>7.80936</td>
      <td>...</td>
      <td>0.017541</td>
      <td>5.07752</td>
      <td>8.06232</td>
      <td>4.88935</td>
      <td>104.545224</td>
      <td>0.586485</td>
      <td>0.137347</td>
      <td>0.586485</td>
      <td>0.137347</td>
      <td>-0.610663</td>
    </tr>
    <tr>
      <th>3</th>
      <td>267.758599</td>
      <td>-32.485136</td>
      <td>886.993</td>
      <td>462.508</td>
      <td>17.1670</td>
      <td>0.0503</td>
      <td>21.99514</td>
      <td>0.18661</td>
      <td>0.15743</td>
      <td>0.66387</td>
      <td>...</td>
      <td>0.007814</td>
      <td>3.24337</td>
      <td>3.98715</td>
      <td>1.99459</td>
      <td>18.891792</td>
      <td>-0.652359</td>
      <td>-0.498145</td>
      <td>-0.652359</td>
      <td>-0.498145</td>
      <td>-0.029480</td>
    </tr>
    <tr>
      <th>4</th>
      <td>267.797234</td>
      <td>-32.472325</td>
      <td>1436.175</td>
      <td>679.620</td>
      <td>12.2333</td>
      <td>0.0034</td>
      <td>15.69679</td>
      <td>0.00462</td>
      <td>0.00248</td>
      <td>3.85503</td>
      <td>...</td>
      <td>0.006415</td>
      <td>4.39385</td>
      <td>4.75353</td>
      <td>3.86889</td>
      <td>71.308296</td>
      <td>0.060235</td>
      <td>-0.349633</td>
      <td>0.060235</td>
      <td>-0.349633</td>
      <td>0.261898</td>
    </tr>
    <tr>
      <th>5</th>
      <td>267.767202</td>
      <td>-32.368453</td>
      <td>1003.500</td>
      <td>2430.221</td>
      <td>15.2115</td>
      <td>0.0146</td>
      <td>18.97295</td>
      <td>0.02625</td>
      <td>0.01531</td>
      <td>5.04427</td>
      <td>...</td>
      <td>0.038873</td>
      <td>4.48847</td>
      <td>4.89371</td>
      <td>3.91457</td>
      <td>97.136510</td>
      <td>0.355485</td>
      <td>-0.035592</td>
      <td>0.355485</td>
      <td>-0.035592</td>
      <td>-0.109320</td>
    </tr>
    <tr>
      <th>6</th>
      <td>267.768327</td>
      <td>-32.367157</td>
      <td>1019.457</td>
      <td>2452.106</td>
      <td>16.0023</td>
      <td>0.0231</td>
      <td>20.05330</td>
      <td>0.05390</td>
      <td>0.03288</td>
      <td>2.51186</td>
      <td>...</td>
      <td>0.010505</td>
      <td>5.27810</td>
      <td>6.39846</td>
      <td>5.05030</td>
      <td>92.754403</td>
      <td>0.266163</td>
      <td>-0.027926</td>
      <td>0.266163</td>
      <td>-0.027926</td>
      <td>-0.112835</td>
    </tr>
    <tr>
      <th>7</th>
      <td>267.720869</td>
      <td>-32.337827</td>
      <td>341.550</td>
      <td>2945.095</td>
      <td>16.4765</td>
      <td>0.0312</td>
      <td>20.54297</td>
      <td>0.05399</td>
      <td>0.04898</td>
      <td>1.10537</td>
      <td>...</td>
      <td>0.017723</td>
      <td>3.73648</td>
      <td>4.71103</td>
      <td>2.74842</td>
      <td>31.794682</td>
      <td>-0.469403</td>
      <td>-0.028505</td>
      <td>-0.469403</td>
      <td>-0.028505</td>
      <td>-0.486495</td>
    </tr>
    <tr>
      <th>8</th>
      <td>267.782309</td>
      <td>-32.314028</td>
      <td>1216.120</td>
      <td>3348.355</td>
      <td>17.2033</td>
      <td>0.0518</td>
      <td>21.85581</td>
      <td>0.59356</td>
      <td>0.12884</td>
      <td>10.07342</td>
      <td>...</td>
      <td>0.005993</td>
      <td>3.53978</td>
      <td>4.56843</td>
      <td>2.42907</td>
      <td>110.145859</td>
      <td>0.659339</td>
      <td>0.190395</td>
      <td>0.659339</td>
      <td>0.190395</td>
      <td>-0.102460</td>
    </tr>
    <tr>
      <th>9</th>
      <td>267.835602</td>
      <td>-32.319098</td>
      <td>1976.117</td>
      <td>3264.169</td>
      <td>17.4567</td>
      <td>0.0626</td>
      <td>22.73157</td>
      <td>0.33644</td>
      <td>0.29747</td>
      <td>-0.58983</td>
      <td>...</td>
      <td>0.006781</td>
      <td>4.54292</td>
      <td>6.00984</td>
      <td>3.99572</td>
      <td>8.191454</td>
      <td>-0.756770</td>
      <td>0.265002</td>
      <td>-0.756770</td>
      <td>0.265002</td>
      <td>0.161073</td>
    </tr>
  </tbody>
</table>
<p>10 rows × 31 columns</p>
</div>

<h1 id="visualise-data-in-2d">Visualise data in 2D</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="https://drive.google.com/uc?export=view&amp;id=1xTUX02Gmqa0XUvaCcKUcEguCouydVlTv" alt="png" /></p>

<h1 id="visualise-data-in-3d">Visualise data in 3D:</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'x1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'x2'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'z'</span><span class="p">],</span> <span class="n">cmap</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">hot</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"PCA1"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"PCA2"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"PCA3"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>


</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'PCA3')
</code></pre></div></div>

<p><img src="images/the_stars_31_1.png" alt="png" /></p>

<h3 id="lets-consider-some-clustering-algorithms">Let’s consider some clustering algorithms:</h3>

<p>We will instantiates different algorithms</p>

<h1 id="1-k-means">1. K-Means</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>


</code></pre></div></div>

<h2 id="optimal-number-of-clusters-and-cluster-evaluation">Optimal number of clusters and cluster evaluation:</h2>

<p>We shall use <code class="highlighter-rouge">the elbow method</code> to determine the optimal number of clusters in k-means clustering.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span><span class="p">,</span> <span class="n">pdist</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="c1"># Avg. within-cluster sum of squares
</span>
<span class="c1">#
</span><span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>

<span class="n">model1</span> <span class="o">=</span> <span class="p">[</span><span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">]</span>

<span class="c1">#centroids
</span><span class="n">cent_</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">cluster_centers_</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">model1</span><span class="p">]</span>

<span class="c1">#Compute distance between each pair of the two collections of inputs
</span><span class="n">D_k</span> <span class="o">=</span> <span class="p">[</span><span class="n">cdist</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">,</span> <span class="n">centrds</span><span class="p">,</span> <span class="s">'euclidean'</span><span class="p">)</span> <span class="k">for</span> <span class="n">centrds</span> <span class="ow">in</span> <span class="n">cent_</span><span class="p">]</span>

<span class="c1">#compute minimum distance to each centroid
</span><span class="n">dist</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">D</span> <span class="ow">in</span> <span class="n">D_k</span><span class="p">]</span>

<span class="c1">#average distance within cluster-sum
</span><span class="n">avgWithinSS</span> <span class="o">=</span> <span class="p">[</span><span class="nb">sum</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">/</span><span class="n">data_rescaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dist</span><span class="p">]</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p><code class="highlighter-rouge">Let's view elbow curve</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># elbow curve - Avg. within-cluster sum of squares
</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">avgWithinSS</span><span class="p">,</span> <span class="s">'b*-'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Number of clusters'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Average within-cluster sum of squares'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'Average within-cluster sum of squares')
</code></pre></div></div>

<p><img src="images/the_stars_40_1" alt="png" /></p>

<p>It is important to scale the input features before you run K-Means, or else the clusters may be very stretched, and K-Means will perform poorly. However, scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally improves things.</p>

<p>Scaling input features, in this work, will be performed in the <code class="highlighter-rouge">Pipeline</code> using <code class="highlighter-rouge">StandardScaler()</code> class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmeans</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[(</span><span class="s">'preprocessor'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">'model'</span><span class="p">,</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">3</span><span class="p">))])</span>

<span class="n">model1</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">y_kmeans</span> <span class="o">=</span> <span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_kmeans</span>

<span class="n">x</span><span class="o">.</span><span class="n">clusterKM</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([0, 2, 1])
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_kmeans</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="images/the_stars_44_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'x1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'x2'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'z'</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_kmeans</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"PCA1"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"PCA2"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"PCA3"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'PCA3')
</code></pre></div></div>

<p><img src="images/the_stars_46_1" alt="png" /></p>

<p>The figures above shows that, the dataset contain 3 groups of stars.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c1">#x.clusterGMM.value_counts().plot('bar')
</span>

<span class="n">x</span><span class="p">[</span><span class="s">'clusterKM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span>
                                 <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                 <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0    9756
1    5451
2    2308
Name: clusterKM, dtype: int64
</code></pre></div></div>

<p><img src="images/the_stars_48_1.png" alt="png" /></p>

<h2 id="2-gaussian-mixture-model">2 Gaussian Mixture Model:</h2>

<p>A Gaussian mixture model (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid.</p>

<p>This class relies on the Expectation-Maximization (EM) algorithm, which has many similarities with the K-Means algorithm: it also initializes the cluster parameters randomly, then it repeats two steps until convergence.</p>

<p>Just like K-Means, EM can end up converging to poor solutions, so it needs to be run several times, keeping only the
best solution. This is why we set <code class="highlighter-rouge">n_init</code> to 10.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">GaussianMixture</span>
</code></pre></div></div>

<h3 id="selecting-the-number-of-components-in-a-classical-gaussian-mixture-mode">Selecting the number of components in a classical Gaussian Mixture Mode.</h3>

<p>We shall use the Bayesian information criterion (BIC) to select the number of components in a Gaussian Mixture in an efficient way.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">gm_bic</span><span class="o">=</span> <span class="p">[]</span>
<span class="n">gm_score</span><span class="o">=</span><span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">):</span>

    <span class="n">gm</span> <span class="o">=</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">tol</span> <span class="o">=</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">"BIC for number of cluster(s) {}: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">gm</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Log-likelihood score for number of cluster(s) {}: {}"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">gm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"-"</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>

    <span class="n">gm_bic</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">gm</span><span class="o">.</span><span class="n">bic</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">))</span>
    <span class="n">gm_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">data_rescaled</span><span class="p">))</span>



<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"The Gaussian Mixture model BIC </span><span class="se">\n</span><span class="s">for determining number of clusters</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)],</span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">gm_bic</span><span class="p">),</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">150</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Number of clusters"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Log of Gaussian mixture BIC score"</span><span class="p">,</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">fontsize</span> <span class="o">=</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>BIC for number of cluster(s) 1: -1676190.6216027052
Log-likelihood score for number of cluster(s) 1: 47.95530168653835
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 2: -2171495.710339565
Log-likelihood score for number of cluster(s) 2: 62.20019052543439
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 3: -2300486.7314481805
Log-likelihood score for number of cluster(s) 3: 65.9879264203326
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 4: -2353688.393926406
Log-likelihood score for number of cluster(s) 4: 67.6121065450227
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 5: -2399421.322070661
Log-likelihood score for number of cluster(s) 5: 69.02307701130657
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 6: -2413156.6007096865
Log-likelihood score for number of cluster(s) 6: 69.52061186510886
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 7: -2425277.5878507434
Log-likelihood score for number of cluster(s) 7: 69.97206360449583
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 8: -2445482.0796555667
Log-likelihood score for number of cluster(s) 8: 70.65427482614848
----------------------------------------------------------------------------------------------------
BIC for number of cluster(s) 9: -2447326.030156673
Log-likelihood score for number of cluster(s) 9: 70.81234841423796
----------------------------------------------------------------------------------------------------
</code></pre></div></div>

<p><img src="images/the_stars_52_1.png" alt="png" /></p>

<p><code class="highlighter-rouge">We shall take 3, as the optimal number of cluster: n_components = 3</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s">'ignore'</span><span class="p">)</span>


<span class="n">GMM</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">steps</span> <span class="o">=</span> <span class="p">[(</span><span class="s">'preprocessor'</span><span class="p">,</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">StandardScaler</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s">'model'</span><span class="p">,</span> <span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">covariance_type</span> <span class="o">=</span> <span class="s">'full'</span><span class="p">))])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GMM</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1">#print(model.score(x))
</span>
<span class="n">y_gmm</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span><span class="p">[</span><span class="s">'clusterGMM'</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_gmm</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

<p>Check whether or not the algorithm converged and how many iterations it took:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="p">[</span><span class="n">GaussianMixture</span><span class="p">(</span><span class="n">n_components</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">n_init</span> <span class="o">=</span> <span class="n">i</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
                     <span class="n">covariance_type</span> <span class="o">=</span> <span class="s">'full'</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">converged_</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)]</span>
<span class="n">l</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[True, True, True, True, True, True, True, True, True]
</code></pre></div></div>

<p>The algorithm took one iteration to converge.</p>

<p><code class="highlighter-rouge">Let's view our 2D &amp; 3D figures:</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'PCA1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'PCA2'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_gmm</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'PCA1'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PCA2'</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'PCA2')
</code></pre></div></div>

<p><img src="images/the_stars_60_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s">'x1'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'x2'</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="s">'z'</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="n">y_gmm</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="s">'viridis'</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"PCA1"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"PCA2"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"PCA3"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0.5, 0, 'PCA3')
</code></pre></div></div>

<p><img src="images/the_stars_61_1.png" alt="png" /></p>

<p>The figure above shows that, the dataset contain 3 groups of stars.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">clusterGMM</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="c1">#x.clusterGMM.value_counts().plot('bar')
</span>

<span class="n">x</span><span class="p">[</span><span class="s">'clusterGMM'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span> <span class="o">=</span> <span class="s">'pie'</span><span class="p">,</span>
                                 <span class="n">explode</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">autopct</span><span class="o">=</span><span class="s">'</span><span class="si">%1.2</span><span class="s">f</span><span class="si">%%</span><span class="s">'</span><span class="p">,</span>
                                 <span class="n">shadow</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                                 <span class="n">legend</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1    9756
2    5451
0    2308
Name: clusterGMM, dtype: int64
</code></pre></div></div>

<p><img src="images/the_stars_63_1.png" alt="png" /></p>

<h3 id="conclusion-from-machine-model">Conclusion from machine model:</h3>

<p>Both models performs better!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>
:ET